{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liBAc53MtsRJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy import arange\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('trucking_data.csv', index_col = 'year')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "ovklUIrht4B7",
        "outputId": "84d73df3-8442-458c-deb4-9ab7802d5ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      lbr_prod  lbr_prod_pctchg  lbr_output  lbr_output_pctchg  emp_trk_trans  \\\n",
              "year                                                                            \n",
              "1960       NaN              NaN         NaN                NaN            NaN   \n",
              "1970       NaN              NaN         NaN                NaN            NaN   \n",
              "1980       NaN              NaN         NaN                NaN            NaN   \n",
              "1981       NaN              NaN         NaN                NaN            NaN   \n",
              "1982       NaN              NaN         NaN                NaN            NaN   \n",
              "\n",
              "      emp_cour_msg  emp_drv_sls  est_total     regist  freight_val  ...  \\\n",
              "year                                                                ...   \n",
              "1960           NaN          NaN        NaN        NaN          NaN  ...   \n",
              "1970           NaN          NaN    64756.0  4586487.0          NaN  ...   \n",
              "1980           NaN          NaN    69796.0  5790653.0          NaN  ...   \n",
              "1981           NaN          NaN        NaN        NaN          NaN  ...   \n",
              "1982           NaN          NaN        NaN        NaN          NaN  ...   \n",
              "\n",
              "      fuel_avg_per_unit  fuel_avg_mpg  fatalities  fatality_rate_permi  \\\n",
              "year                                                                     \n",
              "1960             1333.0           NaN         NaN                  NaN   \n",
              "1970             2467.0           5.5         NaN                  NaN   \n",
              "1980             3447.0           5.4      1262.0                 1.16   \n",
              "1981                NaN           NaN         NaN                  NaN   \n",
              "1982                NaN           NaN         NaN                  NaN   \n",
              "\n",
              "      fatality_rate_pertruck  vehicle_inv_permi  vehicle_inv_pertruck  \\\n",
              "year                                                                    \n",
              "1960                     NaN                NaN                   NaN   \n",
              "1970                     NaN                NaN                   NaN   \n",
              "1980                    2.18               4.96                  9.29   \n",
              "1981                     NaN                NaN                   NaN   \n",
              "1982                     NaN                NaN                   NaN   \n",
              "\n",
              "      miles_per_emp  miles_per_est  emp_per_est  \n",
              "year                                             \n",
              "1960            NaN            NaN          NaN  \n",
              "1970            NaN   9.607604e+11          NaN  \n",
              "1980            NaN   1.554401e+12          NaN  \n",
              "1981            NaN            NaN          NaN  \n",
              "1982            NaN            NaN          NaN  \n",
              "\n",
              "[5 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe4f573d-54c3-4bf8-8c7c-24ab896697ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lbr_prod</th>\n",
              "      <th>lbr_prod_pctchg</th>\n",
              "      <th>lbr_output</th>\n",
              "      <th>lbr_output_pctchg</th>\n",
              "      <th>emp_trk_trans</th>\n",
              "      <th>emp_cour_msg</th>\n",
              "      <th>emp_drv_sls</th>\n",
              "      <th>est_total</th>\n",
              "      <th>regist</th>\n",
              "      <th>freight_val</th>\n",
              "      <th>...</th>\n",
              "      <th>fuel_avg_per_unit</th>\n",
              "      <th>fuel_avg_mpg</th>\n",
              "      <th>fatalities</th>\n",
              "      <th>fatality_rate_permi</th>\n",
              "      <th>fatality_rate_pertruck</th>\n",
              "      <th>vehicle_inv_permi</th>\n",
              "      <th>vehicle_inv_pertruck</th>\n",
              "      <th>miles_per_emp</th>\n",
              "      <th>miles_per_est</th>\n",
              "      <th>emp_per_est</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1333.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64756.0</td>\n",
              "      <td>4586487.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2467.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.607604e+11</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69796.0</td>\n",
              "      <td>5790653.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>3447.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>1262.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>2.18</td>\n",
              "      <td>4.96</td>\n",
              "      <td>9.29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.554401e+12</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1981</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1982</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe4f573d-54c3-4bf8-8c7c-24ab896697ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe4f573d-54c3-4bf8-8c7c-24ab896697ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe4f573d-54c3-4bf8-8c7c-24ab896697ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOR8QNST95WM",
        "outputId": "e7f0475d-a64a-4ab1-a48b-897cbd6c5ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lbr_prod', 'lbr_prod_pctchg', 'lbr_output', 'lbr_output_pctchg',\n",
              "       'emp_trk_trans', 'emp_cour_msg', 'emp_drv_sls', 'est_total', 'regist',\n",
              "       'freight_val', 'freight_val_pctchg', 'freight_val_pcttotal',\n",
              "       'freight_tons', 'freight_tons_pctchg', 'freight_tons_pcttotal',\n",
              "       'freight_tonmi', 'freight_tonmi_pctchg', 'freight_tonmi_pcttotal',\n",
              "       'cptm', 'tax', 'oper_rev', 'oper_exp', 'miles_total',\n",
              "       'miles_avg_per_unit', 'fuel_total', 'fuel_avg_per_unit', 'fuel_avg_mpg',\n",
              "       'fatalities', 'fatality_rate_permi', 'fatality_rate_pertruck',\n",
              "       'vehicle_inv_permi', 'vehicle_inv_pertruck', 'miles_per_emp',\n",
              "       'miles_per_est', 'emp_per_est'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "0lKdL37Guhu_",
        "outputId": "6bd0c4c6-1a87-4fcb-b5cc-11c0eb1fabd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        lbr_prod  lbr_prod_pctchg  lbr_output  \\\n",
              "lbr_prod                1.000000         0.088653    0.993344   \n",
              "lbr_prod_pctchg         0.088653         1.000000    0.056056   \n",
              "lbr_output              0.993344         0.056056    1.000000   \n",
              "lbr_output_pctchg       0.107638         0.951074    0.085500   \n",
              "emp_trk_trans           0.705874        -0.133521    0.707328   \n",
              "emp_cour_msg            0.700313        -0.014597    0.664864   \n",
              "emp_drv_sls             0.461892        -0.156394    0.419735   \n",
              "est_total               0.835909        -0.201713    0.839772   \n",
              "regist                  0.919242        -0.011801    0.919082   \n",
              "freight_val             0.939835         0.025519    0.955963   \n",
              "freight_val_pctchg     -0.447173        -0.581532   -0.445976   \n",
              "freight_val_pcttotal   -0.452366        -0.587872   -0.491014   \n",
              "freight_tons            0.807635         0.460306    0.832119   \n",
              "freight_tons_pctchg    -0.329844         0.751982   -0.362685   \n",
              "freight_tons_pcttotal   0.795929         0.129855    0.840436   \n",
              "freight_tonmi           0.605757        -0.050815    0.628709   \n",
              "freight_tonmi_pctchg   -0.271934        -0.139746   -0.268072   \n",
              "freight_tonmi_pcttotal -0.281143         0.691535   -0.283871   \n",
              "cptm                    0.935846        -0.073243    0.949348   \n",
              "tax                     0.949583        -0.305254    0.955858   \n",
              "oper_rev                0.962699        -0.064364    0.967619   \n",
              "oper_exp                0.890409         0.052077    0.872684   \n",
              "miles_total             0.857310        -0.012053    0.876106   \n",
              "miles_avg_per_unit     -0.443355        -0.011832   -0.392208   \n",
              "fuel_total              0.834889         0.001663    0.858412   \n",
              "fuel_avg_per_unit      -0.671799         0.030228   -0.632547   \n",
              "fuel_avg_mpg            0.597159        -0.063679    0.600412   \n",
              "fatalities              0.405847        -0.011432    0.379259   \n",
              "fatality_rate_permi    -0.594581         0.052281   -0.632285   \n",
              "fatality_rate_pertruck -0.698941         0.082117   -0.720472   \n",
              "vehicle_inv_permi      -0.757771         0.052475   -0.791350   \n",
              "vehicle_inv_pertruck   -0.817166         0.081672   -0.836376   \n",
              "miles_per_emp           0.376718         0.098444    0.420454   \n",
              "miles_per_est           0.653261         0.044368    0.677214   \n",
              "emp_per_est            -0.859579        -0.017111   -0.861883   \n",
              "\n",
              "                        lbr_output_pctchg  emp_trk_trans  emp_cour_msg  \\\n",
              "lbr_prod                         0.107638       0.705874      0.700313   \n",
              "lbr_prod_pctchg                  0.951074      -0.133521     -0.014597   \n",
              "lbr_output                       0.085500       0.707328      0.664864   \n",
              "lbr_output_pctchg                1.000000      -0.132887     -0.054735   \n",
              "emp_trk_trans                   -0.132887       1.000000      0.773193   \n",
              "emp_cour_msg                    -0.054735       0.773193      1.000000   \n",
              "emp_drv_sls                     -0.184988       0.893084      0.789986   \n",
              "est_total                       -0.157473       0.907816      0.906925   \n",
              "regist                          -0.034987       0.663253      0.704046   \n",
              "freight_val                     -0.448476       0.687130      0.498809   \n",
              "freight_val_pctchg              -0.493125      -0.083389     -0.584782   \n",
              "freight_val_pcttotal            -0.387675      -0.448850     -0.390979   \n",
              "freight_tons                     0.134912       0.994065      0.756098   \n",
              "freight_tons_pctchg              0.816964       0.090069      0.257306   \n",
              "freight_tons_pcttotal           -0.514273       0.523157      0.319362   \n",
              "freight_tonmi                   -0.030346       0.599018      0.473132   \n",
              "freight_tonmi_pctchg            -0.202673      -0.391955     -0.423190   \n",
              "freight_tonmi_pcttotal           0.767597      -0.257891     -0.013370   \n",
              "cptm                            -0.069141       0.638342      0.574160   \n",
              "tax                             -0.252312       0.931141      0.909000   \n",
              "oper_rev                        -0.123299       0.806989      0.779039   \n",
              "oper_exp                        -0.030564       0.837436      0.850000   \n",
              "miles_total                     -0.003580       0.605822      0.552201   \n",
              "miles_avg_per_unit               0.099632      -0.318348     -0.565047   \n",
              "fuel_total                       0.011547       0.627472      0.511801   \n",
              "fuel_avg_per_unit                0.110600      -0.381377     -0.671424   \n",
              "fuel_avg_mpg                    -0.037543       0.255408      0.431540   \n",
              "fatalities                      -0.019109       0.704852      0.597472   \n",
              "fatality_rate_permi              0.013349      -0.241437     -0.216260   \n",
              "fatality_rate_pertruck           0.066455      -0.284492     -0.319471   \n",
              "vehicle_inv_permi                0.035380      -0.420252     -0.340507   \n",
              "vehicle_inv_pertruck             0.076445      -0.425770     -0.405917   \n",
              "miles_per_emp                    0.040685      -0.069084      0.058335   \n",
              "miles_per_est                    0.047893       0.233375      0.193486   \n",
              "emp_per_est                      0.039406      -0.371789     -0.528807   \n",
              "\n",
              "                        emp_drv_sls  est_total    regist  freight_val  ...  \\\n",
              "lbr_prod                   0.461892   0.835909  0.919242     0.939835  ...   \n",
              "lbr_prod_pctchg           -0.156394  -0.201713 -0.011801     0.025519  ...   \n",
              "lbr_output                 0.419735   0.839772  0.919082     0.955963  ...   \n",
              "lbr_output_pctchg         -0.184988  -0.157473 -0.034987    -0.448476  ...   \n",
              "emp_trk_trans              0.893084   0.907816  0.663253     0.687130  ...   \n",
              "emp_cour_msg               0.789986   0.906925  0.704046     0.498809  ...   \n",
              "emp_drv_sls                1.000000   0.895897  0.439315     0.049709  ...   \n",
              "est_total                  0.895897   1.000000  0.813994     0.692921  ...   \n",
              "regist                     0.439315   0.813994  1.000000     0.954997  ...   \n",
              "freight_val                0.049709   0.692921  0.954997     1.000000  ...   \n",
              "freight_val_pctchg        -0.296376  -0.242900 -0.180206    -0.188874  ...   \n",
              "freight_val_pcttotal      -0.728452  -0.448877 -0.261336    -0.441590  ...   \n",
              "freight_tons               0.815864   0.969554  0.894936     0.803326  ...   \n",
              "freight_tons_pctchg        0.962747   0.113002 -0.306850    -0.545710  ...   \n",
              "freight_tons_pcttotal      0.001211   0.524682  0.818487     0.898131  ...   \n",
              "freight_tonmi              0.563521   0.730308  0.390074     0.428350  ...   \n",
              "freight_tonmi_pctchg      -0.036664  -0.502386 -0.376514    -0.004230  ...   \n",
              "freight_tonmi_pcttotal     0.357225  -0.242926 -0.664108    -0.458304  ...   \n",
              "cptm                       0.228122   0.726887  0.903482     0.982925  ...   \n",
              "tax                        0.913024   0.984420  0.920284     0.992944  ...   \n",
              "oper_rev                   0.659455   0.898849  0.950479     0.966962  ...   \n",
              "oper_exp                   0.731116   0.878683  0.851124     0.631376  ...   \n",
              "miles_total                0.239006   0.870630  0.944263     0.862239  ...   \n",
              "miles_avg_per_unit        -0.523121   0.632347  0.218542    -0.653789  ...   \n",
              "fuel_total                 0.185672   0.871309  0.923196     0.850857  ...   \n",
              "fuel_avg_per_unit         -0.504121   0.417154 -0.075900    -0.701849  ...   \n",
              "fuel_avg_mpg               0.199388   0.724669  0.725010     0.485919  ...   \n",
              "fatalities                 0.867637  -0.223507 -0.037612     0.519792  ...   \n",
              "fatality_rate_permi        0.506594  -0.758980 -0.561814    -0.793770  ...   \n",
              "fatality_rate_pertruck     0.367880  -0.773108 -0.713140    -0.950924  ...   \n",
              "vehicle_inv_permi          0.343080  -0.801980 -0.779739    -0.923391  ...   \n",
              "vehicle_inv_pertruck       0.233576  -0.735997 -0.913235    -0.972356  ...   \n",
              "miles_per_emp             -0.243179   0.023180  0.693731     0.812604  ...   \n",
              "miles_per_est             -0.334054   0.632946  0.850378     0.869147  ...   \n",
              "emp_per_est               -0.109878  -0.539477 -0.832893    -0.994634  ...   \n",
              "\n",
              "                        fuel_avg_per_unit  fuel_avg_mpg  fatalities  \\\n",
              "lbr_prod                        -0.671799      0.597159    0.405847   \n",
              "lbr_prod_pctchg                  0.030228     -0.063679   -0.011432   \n",
              "lbr_output                      -0.632547      0.600412    0.379259   \n",
              "lbr_output_pctchg                0.110600     -0.037543   -0.019109   \n",
              "emp_trk_trans                   -0.381377      0.255408    0.704852   \n",
              "emp_cour_msg                    -0.671424      0.431540    0.597472   \n",
              "emp_drv_sls                     -0.504121      0.199388    0.867637   \n",
              "est_total                        0.417154      0.724669   -0.223507   \n",
              "regist                          -0.075900      0.725010   -0.037612   \n",
              "freight_val                     -0.701849      0.485919    0.519792   \n",
              "freight_val_pctchg               0.736476     -0.390220   -0.420660   \n",
              "freight_val_pcttotal             0.403589     -0.806027   -0.678677   \n",
              "freight_tons                    -0.410916      0.479218    0.879785   \n",
              "freight_tons_pctchg              0.107569      0.249000    0.395475   \n",
              "freight_tons_pcttotal           -0.845902      0.841050    0.490517   \n",
              "freight_tonmi                    0.331738      0.390908   -0.163691   \n",
              "freight_tonmi_pctchg            -0.159652     -0.173650    0.020537   \n",
              "freight_tonmi_pcttotal           0.733453     -0.693247   -0.163080   \n",
              "cptm                            -0.614040      0.514858    0.235538   \n",
              "tax                              0.578889      0.724949   -0.207450   \n",
              "oper_rev                        -0.705208      0.569239    0.495548   \n",
              "oper_exp                        -0.821713      0.404212    0.663815   \n",
              "miles_total                      0.366765      0.778542   -0.248879   \n",
              "miles_avg_per_unit               0.947354      0.470583   -0.712288   \n",
              "fuel_total                       0.483408      0.695198   -0.254598   \n",
              "fuel_avg_per_unit                1.000000      0.060018   -0.531425   \n",
              "fuel_avg_mpg                     0.060018      1.000000   -0.303609   \n",
              "fatalities                      -0.531425     -0.303609    1.000000   \n",
              "fatality_rate_permi             -0.276554     -0.651067    0.768112   \n",
              "fatality_rate_pertruck          -0.178797     -0.681199    0.729886   \n",
              "vehicle_inv_permi               -0.068799     -0.703426    0.592716   \n",
              "vehicle_inv_pertruck             0.121891     -0.656917    0.432180   \n",
              "miles_per_emp                   -0.308729      0.388550   -0.351354   \n",
              "miles_per_est                    0.302632      0.665633   -0.321771   \n",
              "emp_per_est                      0.810949     -0.332963   -0.157203   \n",
              "\n",
              "                        fatality_rate_permi  fatality_rate_pertruck  \\\n",
              "lbr_prod                          -0.594581               -0.698941   \n",
              "lbr_prod_pctchg                    0.052281                0.082117   \n",
              "lbr_output                        -0.632285               -0.720472   \n",
              "lbr_output_pctchg                  0.013349                0.066455   \n",
              "emp_trk_trans                     -0.241437               -0.284492   \n",
              "emp_cour_msg                      -0.216260               -0.319471   \n",
              "emp_drv_sls                        0.506594                0.367880   \n",
              "est_total                         -0.758980               -0.773108   \n",
              "regist                            -0.561814               -0.713140   \n",
              "freight_val                       -0.793770               -0.950924   \n",
              "freight_val_pctchg                -0.446104               -0.102552   \n",
              "freight_val_pcttotal              -0.125685               -0.133234   \n",
              "freight_tons                      -0.648162               -0.658117   \n",
              "freight_tons_pctchg                0.629991                0.698488   \n",
              "freight_tons_pcttotal             -0.595956               -0.747816   \n",
              "freight_tonmi                     -0.532580               -0.471971   \n",
              "freight_tonmi_pctchg               0.488943                0.417474   \n",
              "freight_tonmi_pcttotal             0.623798                0.755645   \n",
              "cptm                              -0.679831               -0.785563   \n",
              "tax                               -0.755205               -0.851206   \n",
              "oper_rev                          -0.552619               -0.676140   \n",
              "oper_exp                           0.167046               -0.082041   \n",
              "miles_total                       -0.729745               -0.829353   \n",
              "miles_avg_per_unit                -0.646871               -0.560652   \n",
              "fuel_total                        -0.727835               -0.824030   \n",
              "fuel_avg_per_unit                 -0.276554               -0.178797   \n",
              "fuel_avg_mpg                      -0.651067               -0.681199   \n",
              "fatalities                         0.768112                0.729886   \n",
              "fatality_rate_permi                1.000000                0.975832   \n",
              "fatality_rate_pertruck             0.975832                1.000000   \n",
              "vehicle_inv_permi                  0.926984                0.970407   \n",
              "vehicle_inv_pertruck               0.773831                0.881461   \n",
              "miles_per_emp                     -0.896857               -0.919147   \n",
              "miles_per_est                     -0.569843               -0.695070   \n",
              "emp_per_est                        0.360457                0.597532   \n",
              "\n",
              "                        vehicle_inv_permi  vehicle_inv_pertruck  \\\n",
              "lbr_prod                        -0.757771             -0.817166   \n",
              "lbr_prod_pctchg                  0.052475              0.081672   \n",
              "lbr_output                      -0.791350             -0.836376   \n",
              "lbr_output_pctchg                0.035380              0.076445   \n",
              "emp_trk_trans                   -0.420252             -0.425770   \n",
              "emp_cour_msg                    -0.340507             -0.405917   \n",
              "emp_drv_sls                      0.343080              0.233576   \n",
              "est_total                       -0.801980             -0.735997   \n",
              "regist                          -0.779739             -0.913235   \n",
              "freight_val                     -0.923391             -0.972356   \n",
              "freight_val_pctchg              -0.198407             -0.045232   \n",
              "freight_val_pcttotal            -0.002044             -0.015834   \n",
              "freight_tons                    -0.745652             -0.731537   \n",
              "freight_tons_pctchg              0.584454              0.608572   \n",
              "freight_tons_pcttotal           -0.754222             -0.798927   \n",
              "freight_tonmi                   -0.519534             -0.396460   \n",
              "freight_tonmi_pctchg             0.489731              0.422213   \n",
              "freight_tonmi_pcttotal           0.730401              0.768312   \n",
              "cptm                            -0.814723             -0.870757   \n",
              "tax                             -0.910633             -0.967548   \n",
              "oper_rev                        -0.728934             -0.804381   \n",
              "oper_exp                        -0.053297             -0.274149   \n",
              "miles_total                     -0.911015             -0.959166   \n",
              "miles_avg_per_unit              -0.477597             -0.273873   \n",
              "fuel_total                      -0.910871             -0.957870   \n",
              "fuel_avg_per_unit               -0.068799              0.121891   \n",
              "fuel_avg_mpg                    -0.703426             -0.656917   \n",
              "fatalities                       0.592716              0.432180   \n",
              "fatality_rate_permi              0.926984              0.773831   \n",
              "fatality_rate_pertruck           0.970407              0.881461   \n",
              "vehicle_inv_permi                1.000000              0.948057   \n",
              "vehicle_inv_pertruck             0.948057              1.000000   \n",
              "miles_per_emp                   -0.972071             -0.942649   \n",
              "miles_per_est                   -0.798228             -0.901828   \n",
              "emp_per_est                      0.556885              0.711926   \n",
              "\n",
              "                        miles_per_emp  miles_per_est  emp_per_est  \n",
              "lbr_prod                     0.376718       0.653261    -0.859579  \n",
              "lbr_prod_pctchg              0.098444       0.044368    -0.017111  \n",
              "lbr_output                   0.420454       0.677214    -0.861883  \n",
              "lbr_output_pctchg            0.040685       0.047893     0.039406  \n",
              "emp_trk_trans               -0.069084       0.233375    -0.371789  \n",
              "emp_cour_msg                 0.058335       0.193486    -0.528807  \n",
              "emp_drv_sls                 -0.243179      -0.334054    -0.109878  \n",
              "est_total                    0.023180       0.632946    -0.539477  \n",
              "regist                       0.693731       0.850378    -0.832893  \n",
              "freight_val                  0.812604       0.869147    -0.994634  \n",
              "freight_val_pctchg          -0.098643       0.231257     0.536316  \n",
              "freight_val_pcttotal        -0.802172      -0.211991     0.609751  \n",
              "freight_tons                 0.719101       0.836306    -0.588025  \n",
              "freight_tons_pctchg          0.134815      -0.385509     0.079847  \n",
              "freight_tons_pcttotal        0.918658       0.771518    -0.983888  \n",
              "freight_tonmi               -0.722835       0.210613     0.586787  \n",
              "freight_tonmi_pctchg        -0.193914      -0.375601     0.046546  \n",
              "freight_tonmi_pcttotal      -0.803277      -0.694362     0.879348  \n",
              "cptm                         0.613558       0.723034    -0.838482  \n",
              "tax                          0.755111       0.838139    -0.971243  \n",
              "oper_rev                     0.364194       0.634314    -0.790067  \n",
              "oper_exp                     0.133577      -0.007116    -0.675843  \n",
              "miles_total                  0.883259       0.921625    -0.647182  \n",
              "miles_avg_per_unit          -0.112575       0.533391     0.765142  \n",
              "fuel_total                   0.874104       0.915559    -0.626848  \n",
              "fuel_avg_per_unit           -0.308729       0.302632     0.810949  \n",
              "fuel_avg_mpg                 0.388550       0.665633    -0.332963  \n",
              "fatalities                  -0.351354      -0.321771    -0.157203  \n",
              "fatality_rate_permi         -0.896857      -0.569843     0.360457  \n",
              "fatality_rate_pertruck      -0.919147      -0.695070     0.597532  \n",
              "vehicle_inv_permi           -0.972071      -0.798228     0.556885  \n",
              "vehicle_inv_pertruck        -0.942649      -0.901828     0.711926  \n",
              "miles_per_emp                1.000000       0.977969    -0.589257  \n",
              "miles_per_est                0.977969       1.000000    -0.408253  \n",
              "emp_per_est                 -0.589257      -0.408253     1.000000  \n",
              "\n",
              "[35 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9efd9d45-8d7f-4c9a-9910-b7087b848eb6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lbr_prod</th>\n",
              "      <th>lbr_prod_pctchg</th>\n",
              "      <th>lbr_output</th>\n",
              "      <th>lbr_output_pctchg</th>\n",
              "      <th>emp_trk_trans</th>\n",
              "      <th>emp_cour_msg</th>\n",
              "      <th>emp_drv_sls</th>\n",
              "      <th>est_total</th>\n",
              "      <th>regist</th>\n",
              "      <th>freight_val</th>\n",
              "      <th>...</th>\n",
              "      <th>fuel_avg_per_unit</th>\n",
              "      <th>fuel_avg_mpg</th>\n",
              "      <th>fatalities</th>\n",
              "      <th>fatality_rate_permi</th>\n",
              "      <th>fatality_rate_pertruck</th>\n",
              "      <th>vehicle_inv_permi</th>\n",
              "      <th>vehicle_inv_pertruck</th>\n",
              "      <th>miles_per_emp</th>\n",
              "      <th>miles_per_est</th>\n",
              "      <th>emp_per_est</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lbr_prod</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.088653</td>\n",
              "      <td>0.993344</td>\n",
              "      <td>0.107638</td>\n",
              "      <td>0.705874</td>\n",
              "      <td>0.700313</td>\n",
              "      <td>0.461892</td>\n",
              "      <td>0.835909</td>\n",
              "      <td>0.919242</td>\n",
              "      <td>0.939835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.671799</td>\n",
              "      <td>0.597159</td>\n",
              "      <td>0.405847</td>\n",
              "      <td>-0.594581</td>\n",
              "      <td>-0.698941</td>\n",
              "      <td>-0.757771</td>\n",
              "      <td>-0.817166</td>\n",
              "      <td>0.376718</td>\n",
              "      <td>0.653261</td>\n",
              "      <td>-0.859579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lbr_prod_pctchg</th>\n",
              "      <td>0.088653</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.056056</td>\n",
              "      <td>0.951074</td>\n",
              "      <td>-0.133521</td>\n",
              "      <td>-0.014597</td>\n",
              "      <td>-0.156394</td>\n",
              "      <td>-0.201713</td>\n",
              "      <td>-0.011801</td>\n",
              "      <td>0.025519</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030228</td>\n",
              "      <td>-0.063679</td>\n",
              "      <td>-0.011432</td>\n",
              "      <td>0.052281</td>\n",
              "      <td>0.082117</td>\n",
              "      <td>0.052475</td>\n",
              "      <td>0.081672</td>\n",
              "      <td>0.098444</td>\n",
              "      <td>0.044368</td>\n",
              "      <td>-0.017111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lbr_output</th>\n",
              "      <td>0.993344</td>\n",
              "      <td>0.056056</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>0.707328</td>\n",
              "      <td>0.664864</td>\n",
              "      <td>0.419735</td>\n",
              "      <td>0.839772</td>\n",
              "      <td>0.919082</td>\n",
              "      <td>0.955963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.632547</td>\n",
              "      <td>0.600412</td>\n",
              "      <td>0.379259</td>\n",
              "      <td>-0.632285</td>\n",
              "      <td>-0.720472</td>\n",
              "      <td>-0.791350</td>\n",
              "      <td>-0.836376</td>\n",
              "      <td>0.420454</td>\n",
              "      <td>0.677214</td>\n",
              "      <td>-0.861883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lbr_output_pctchg</th>\n",
              "      <td>0.107638</td>\n",
              "      <td>0.951074</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.132887</td>\n",
              "      <td>-0.054735</td>\n",
              "      <td>-0.184988</td>\n",
              "      <td>-0.157473</td>\n",
              "      <td>-0.034987</td>\n",
              "      <td>-0.448476</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110600</td>\n",
              "      <td>-0.037543</td>\n",
              "      <td>-0.019109</td>\n",
              "      <td>0.013349</td>\n",
              "      <td>0.066455</td>\n",
              "      <td>0.035380</td>\n",
              "      <td>0.076445</td>\n",
              "      <td>0.040685</td>\n",
              "      <td>0.047893</td>\n",
              "      <td>0.039406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emp_trk_trans</th>\n",
              "      <td>0.705874</td>\n",
              "      <td>-0.133521</td>\n",
              "      <td>0.707328</td>\n",
              "      <td>-0.132887</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.773193</td>\n",
              "      <td>0.893084</td>\n",
              "      <td>0.907816</td>\n",
              "      <td>0.663253</td>\n",
              "      <td>0.687130</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.381377</td>\n",
              "      <td>0.255408</td>\n",
              "      <td>0.704852</td>\n",
              "      <td>-0.241437</td>\n",
              "      <td>-0.284492</td>\n",
              "      <td>-0.420252</td>\n",
              "      <td>-0.425770</td>\n",
              "      <td>-0.069084</td>\n",
              "      <td>0.233375</td>\n",
              "      <td>-0.371789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emp_cour_msg</th>\n",
              "      <td>0.700313</td>\n",
              "      <td>-0.014597</td>\n",
              "      <td>0.664864</td>\n",
              "      <td>-0.054735</td>\n",
              "      <td>0.773193</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.789986</td>\n",
              "      <td>0.906925</td>\n",
              "      <td>0.704046</td>\n",
              "      <td>0.498809</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.671424</td>\n",
              "      <td>0.431540</td>\n",
              "      <td>0.597472</td>\n",
              "      <td>-0.216260</td>\n",
              "      <td>-0.319471</td>\n",
              "      <td>-0.340507</td>\n",
              "      <td>-0.405917</td>\n",
              "      <td>0.058335</td>\n",
              "      <td>0.193486</td>\n",
              "      <td>-0.528807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emp_drv_sls</th>\n",
              "      <td>0.461892</td>\n",
              "      <td>-0.156394</td>\n",
              "      <td>0.419735</td>\n",
              "      <td>-0.184988</td>\n",
              "      <td>0.893084</td>\n",
              "      <td>0.789986</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.895897</td>\n",
              "      <td>0.439315</td>\n",
              "      <td>0.049709</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.504121</td>\n",
              "      <td>0.199388</td>\n",
              "      <td>0.867637</td>\n",
              "      <td>0.506594</td>\n",
              "      <td>0.367880</td>\n",
              "      <td>0.343080</td>\n",
              "      <td>0.233576</td>\n",
              "      <td>-0.243179</td>\n",
              "      <td>-0.334054</td>\n",
              "      <td>-0.109878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>est_total</th>\n",
              "      <td>0.835909</td>\n",
              "      <td>-0.201713</td>\n",
              "      <td>0.839772</td>\n",
              "      <td>-0.157473</td>\n",
              "      <td>0.907816</td>\n",
              "      <td>0.906925</td>\n",
              "      <td>0.895897</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.813994</td>\n",
              "      <td>0.692921</td>\n",
              "      <td>...</td>\n",
              "      <td>0.417154</td>\n",
              "      <td>0.724669</td>\n",
              "      <td>-0.223507</td>\n",
              "      <td>-0.758980</td>\n",
              "      <td>-0.773108</td>\n",
              "      <td>-0.801980</td>\n",
              "      <td>-0.735997</td>\n",
              "      <td>0.023180</td>\n",
              "      <td>0.632946</td>\n",
              "      <td>-0.539477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>regist</th>\n",
              "      <td>0.919242</td>\n",
              "      <td>-0.011801</td>\n",
              "      <td>0.919082</td>\n",
              "      <td>-0.034987</td>\n",
              "      <td>0.663253</td>\n",
              "      <td>0.704046</td>\n",
              "      <td>0.439315</td>\n",
              "      <td>0.813994</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954997</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.075900</td>\n",
              "      <td>0.725010</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>-0.561814</td>\n",
              "      <td>-0.713140</td>\n",
              "      <td>-0.779739</td>\n",
              "      <td>-0.913235</td>\n",
              "      <td>0.693731</td>\n",
              "      <td>0.850378</td>\n",
              "      <td>-0.832893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_val</th>\n",
              "      <td>0.939835</td>\n",
              "      <td>0.025519</td>\n",
              "      <td>0.955963</td>\n",
              "      <td>-0.448476</td>\n",
              "      <td>0.687130</td>\n",
              "      <td>0.498809</td>\n",
              "      <td>0.049709</td>\n",
              "      <td>0.692921</td>\n",
              "      <td>0.954997</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.701849</td>\n",
              "      <td>0.485919</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>-0.793770</td>\n",
              "      <td>-0.950924</td>\n",
              "      <td>-0.923391</td>\n",
              "      <td>-0.972356</td>\n",
              "      <td>0.812604</td>\n",
              "      <td>0.869147</td>\n",
              "      <td>-0.994634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_val_pctchg</th>\n",
              "      <td>-0.447173</td>\n",
              "      <td>-0.581532</td>\n",
              "      <td>-0.445976</td>\n",
              "      <td>-0.493125</td>\n",
              "      <td>-0.083389</td>\n",
              "      <td>-0.584782</td>\n",
              "      <td>-0.296376</td>\n",
              "      <td>-0.242900</td>\n",
              "      <td>-0.180206</td>\n",
              "      <td>-0.188874</td>\n",
              "      <td>...</td>\n",
              "      <td>0.736476</td>\n",
              "      <td>-0.390220</td>\n",
              "      <td>-0.420660</td>\n",
              "      <td>-0.446104</td>\n",
              "      <td>-0.102552</td>\n",
              "      <td>-0.198407</td>\n",
              "      <td>-0.045232</td>\n",
              "      <td>-0.098643</td>\n",
              "      <td>0.231257</td>\n",
              "      <td>0.536316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_val_pcttotal</th>\n",
              "      <td>-0.452366</td>\n",
              "      <td>-0.587872</td>\n",
              "      <td>-0.491014</td>\n",
              "      <td>-0.387675</td>\n",
              "      <td>-0.448850</td>\n",
              "      <td>-0.390979</td>\n",
              "      <td>-0.728452</td>\n",
              "      <td>-0.448877</td>\n",
              "      <td>-0.261336</td>\n",
              "      <td>-0.441590</td>\n",
              "      <td>...</td>\n",
              "      <td>0.403589</td>\n",
              "      <td>-0.806027</td>\n",
              "      <td>-0.678677</td>\n",
              "      <td>-0.125685</td>\n",
              "      <td>-0.133234</td>\n",
              "      <td>-0.002044</td>\n",
              "      <td>-0.015834</td>\n",
              "      <td>-0.802172</td>\n",
              "      <td>-0.211991</td>\n",
              "      <td>0.609751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tons</th>\n",
              "      <td>0.807635</td>\n",
              "      <td>0.460306</td>\n",
              "      <td>0.832119</td>\n",
              "      <td>0.134912</td>\n",
              "      <td>0.994065</td>\n",
              "      <td>0.756098</td>\n",
              "      <td>0.815864</td>\n",
              "      <td>0.969554</td>\n",
              "      <td>0.894936</td>\n",
              "      <td>0.803326</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.410916</td>\n",
              "      <td>0.479218</td>\n",
              "      <td>0.879785</td>\n",
              "      <td>-0.648162</td>\n",
              "      <td>-0.658117</td>\n",
              "      <td>-0.745652</td>\n",
              "      <td>-0.731537</td>\n",
              "      <td>0.719101</td>\n",
              "      <td>0.836306</td>\n",
              "      <td>-0.588025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tons_pctchg</th>\n",
              "      <td>-0.329844</td>\n",
              "      <td>0.751982</td>\n",
              "      <td>-0.362685</td>\n",
              "      <td>0.816964</td>\n",
              "      <td>0.090069</td>\n",
              "      <td>0.257306</td>\n",
              "      <td>0.962747</td>\n",
              "      <td>0.113002</td>\n",
              "      <td>-0.306850</td>\n",
              "      <td>-0.545710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.107569</td>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.395475</td>\n",
              "      <td>0.629991</td>\n",
              "      <td>0.698488</td>\n",
              "      <td>0.584454</td>\n",
              "      <td>0.608572</td>\n",
              "      <td>0.134815</td>\n",
              "      <td>-0.385509</td>\n",
              "      <td>0.079847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tons_pcttotal</th>\n",
              "      <td>0.795929</td>\n",
              "      <td>0.129855</td>\n",
              "      <td>0.840436</td>\n",
              "      <td>-0.514273</td>\n",
              "      <td>0.523157</td>\n",
              "      <td>0.319362</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.524682</td>\n",
              "      <td>0.818487</td>\n",
              "      <td>0.898131</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.845902</td>\n",
              "      <td>0.841050</td>\n",
              "      <td>0.490517</td>\n",
              "      <td>-0.595956</td>\n",
              "      <td>-0.747816</td>\n",
              "      <td>-0.754222</td>\n",
              "      <td>-0.798927</td>\n",
              "      <td>0.918658</td>\n",
              "      <td>0.771518</td>\n",
              "      <td>-0.983888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tonmi</th>\n",
              "      <td>0.605757</td>\n",
              "      <td>-0.050815</td>\n",
              "      <td>0.628709</td>\n",
              "      <td>-0.030346</td>\n",
              "      <td>0.599018</td>\n",
              "      <td>0.473132</td>\n",
              "      <td>0.563521</td>\n",
              "      <td>0.730308</td>\n",
              "      <td>0.390074</td>\n",
              "      <td>0.428350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.331738</td>\n",
              "      <td>0.390908</td>\n",
              "      <td>-0.163691</td>\n",
              "      <td>-0.532580</td>\n",
              "      <td>-0.471971</td>\n",
              "      <td>-0.519534</td>\n",
              "      <td>-0.396460</td>\n",
              "      <td>-0.722835</td>\n",
              "      <td>0.210613</td>\n",
              "      <td>0.586787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tonmi_pctchg</th>\n",
              "      <td>-0.271934</td>\n",
              "      <td>-0.139746</td>\n",
              "      <td>-0.268072</td>\n",
              "      <td>-0.202673</td>\n",
              "      <td>-0.391955</td>\n",
              "      <td>-0.423190</td>\n",
              "      <td>-0.036664</td>\n",
              "      <td>-0.502386</td>\n",
              "      <td>-0.376514</td>\n",
              "      <td>-0.004230</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.159652</td>\n",
              "      <td>-0.173650</td>\n",
              "      <td>0.020537</td>\n",
              "      <td>0.488943</td>\n",
              "      <td>0.417474</td>\n",
              "      <td>0.489731</td>\n",
              "      <td>0.422213</td>\n",
              "      <td>-0.193914</td>\n",
              "      <td>-0.375601</td>\n",
              "      <td>0.046546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freight_tonmi_pcttotal</th>\n",
              "      <td>-0.281143</td>\n",
              "      <td>0.691535</td>\n",
              "      <td>-0.283871</td>\n",
              "      <td>0.767597</td>\n",
              "      <td>-0.257891</td>\n",
              "      <td>-0.013370</td>\n",
              "      <td>0.357225</td>\n",
              "      <td>-0.242926</td>\n",
              "      <td>-0.664108</td>\n",
              "      <td>-0.458304</td>\n",
              "      <td>...</td>\n",
              "      <td>0.733453</td>\n",
              "      <td>-0.693247</td>\n",
              "      <td>-0.163080</td>\n",
              "      <td>0.623798</td>\n",
              "      <td>0.755645</td>\n",
              "      <td>0.730401</td>\n",
              "      <td>0.768312</td>\n",
              "      <td>-0.803277</td>\n",
              "      <td>-0.694362</td>\n",
              "      <td>0.879348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cptm</th>\n",
              "      <td>0.935846</td>\n",
              "      <td>-0.073243</td>\n",
              "      <td>0.949348</td>\n",
              "      <td>-0.069141</td>\n",
              "      <td>0.638342</td>\n",
              "      <td>0.574160</td>\n",
              "      <td>0.228122</td>\n",
              "      <td>0.726887</td>\n",
              "      <td>0.903482</td>\n",
              "      <td>0.982925</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.614040</td>\n",
              "      <td>0.514858</td>\n",
              "      <td>0.235538</td>\n",
              "      <td>-0.679831</td>\n",
              "      <td>-0.785563</td>\n",
              "      <td>-0.814723</td>\n",
              "      <td>-0.870757</td>\n",
              "      <td>0.613558</td>\n",
              "      <td>0.723034</td>\n",
              "      <td>-0.838482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tax</th>\n",
              "      <td>0.949583</td>\n",
              "      <td>-0.305254</td>\n",
              "      <td>0.955858</td>\n",
              "      <td>-0.252312</td>\n",
              "      <td>0.931141</td>\n",
              "      <td>0.909000</td>\n",
              "      <td>0.913024</td>\n",
              "      <td>0.984420</td>\n",
              "      <td>0.920284</td>\n",
              "      <td>0.992944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.578889</td>\n",
              "      <td>0.724949</td>\n",
              "      <td>-0.207450</td>\n",
              "      <td>-0.755205</td>\n",
              "      <td>-0.851206</td>\n",
              "      <td>-0.910633</td>\n",
              "      <td>-0.967548</td>\n",
              "      <td>0.755111</td>\n",
              "      <td>0.838139</td>\n",
              "      <td>-0.971243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oper_rev</th>\n",
              "      <td>0.962699</td>\n",
              "      <td>-0.064364</td>\n",
              "      <td>0.967619</td>\n",
              "      <td>-0.123299</td>\n",
              "      <td>0.806989</td>\n",
              "      <td>0.779039</td>\n",
              "      <td>0.659455</td>\n",
              "      <td>0.898849</td>\n",
              "      <td>0.950479</td>\n",
              "      <td>0.966962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.705208</td>\n",
              "      <td>0.569239</td>\n",
              "      <td>0.495548</td>\n",
              "      <td>-0.552619</td>\n",
              "      <td>-0.676140</td>\n",
              "      <td>-0.728934</td>\n",
              "      <td>-0.804381</td>\n",
              "      <td>0.364194</td>\n",
              "      <td>0.634314</td>\n",
              "      <td>-0.790067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oper_exp</th>\n",
              "      <td>0.890409</td>\n",
              "      <td>0.052077</td>\n",
              "      <td>0.872684</td>\n",
              "      <td>-0.030564</td>\n",
              "      <td>0.837436</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.731116</td>\n",
              "      <td>0.878683</td>\n",
              "      <td>0.851124</td>\n",
              "      <td>0.631376</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.821713</td>\n",
              "      <td>0.404212</td>\n",
              "      <td>0.663815</td>\n",
              "      <td>0.167046</td>\n",
              "      <td>-0.082041</td>\n",
              "      <td>-0.053297</td>\n",
              "      <td>-0.274149</td>\n",
              "      <td>0.133577</td>\n",
              "      <td>-0.007116</td>\n",
              "      <td>-0.675843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miles_total</th>\n",
              "      <td>0.857310</td>\n",
              "      <td>-0.012053</td>\n",
              "      <td>0.876106</td>\n",
              "      <td>-0.003580</td>\n",
              "      <td>0.605822</td>\n",
              "      <td>0.552201</td>\n",
              "      <td>0.239006</td>\n",
              "      <td>0.870630</td>\n",
              "      <td>0.944263</td>\n",
              "      <td>0.862239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.366765</td>\n",
              "      <td>0.778542</td>\n",
              "      <td>-0.248879</td>\n",
              "      <td>-0.729745</td>\n",
              "      <td>-0.829353</td>\n",
              "      <td>-0.911015</td>\n",
              "      <td>-0.959166</td>\n",
              "      <td>0.883259</td>\n",
              "      <td>0.921625</td>\n",
              "      <td>-0.647182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miles_avg_per_unit</th>\n",
              "      <td>-0.443355</td>\n",
              "      <td>-0.011832</td>\n",
              "      <td>-0.392208</td>\n",
              "      <td>0.099632</td>\n",
              "      <td>-0.318348</td>\n",
              "      <td>-0.565047</td>\n",
              "      <td>-0.523121</td>\n",
              "      <td>0.632347</td>\n",
              "      <td>0.218542</td>\n",
              "      <td>-0.653789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.947354</td>\n",
              "      <td>0.470583</td>\n",
              "      <td>-0.712288</td>\n",
              "      <td>-0.646871</td>\n",
              "      <td>-0.560652</td>\n",
              "      <td>-0.477597</td>\n",
              "      <td>-0.273873</td>\n",
              "      <td>-0.112575</td>\n",
              "      <td>0.533391</td>\n",
              "      <td>0.765142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fuel_total</th>\n",
              "      <td>0.834889</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.858412</td>\n",
              "      <td>0.011547</td>\n",
              "      <td>0.627472</td>\n",
              "      <td>0.511801</td>\n",
              "      <td>0.185672</td>\n",
              "      <td>0.871309</td>\n",
              "      <td>0.923196</td>\n",
              "      <td>0.850857</td>\n",
              "      <td>...</td>\n",
              "      <td>0.483408</td>\n",
              "      <td>0.695198</td>\n",
              "      <td>-0.254598</td>\n",
              "      <td>-0.727835</td>\n",
              "      <td>-0.824030</td>\n",
              "      <td>-0.910871</td>\n",
              "      <td>-0.957870</td>\n",
              "      <td>0.874104</td>\n",
              "      <td>0.915559</td>\n",
              "      <td>-0.626848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fuel_avg_per_unit</th>\n",
              "      <td>-0.671799</td>\n",
              "      <td>0.030228</td>\n",
              "      <td>-0.632547</td>\n",
              "      <td>0.110600</td>\n",
              "      <td>-0.381377</td>\n",
              "      <td>-0.671424</td>\n",
              "      <td>-0.504121</td>\n",
              "      <td>0.417154</td>\n",
              "      <td>-0.075900</td>\n",
              "      <td>-0.701849</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.531425</td>\n",
              "      <td>-0.276554</td>\n",
              "      <td>-0.178797</td>\n",
              "      <td>-0.068799</td>\n",
              "      <td>0.121891</td>\n",
              "      <td>-0.308729</td>\n",
              "      <td>0.302632</td>\n",
              "      <td>0.810949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fuel_avg_mpg</th>\n",
              "      <td>0.597159</td>\n",
              "      <td>-0.063679</td>\n",
              "      <td>0.600412</td>\n",
              "      <td>-0.037543</td>\n",
              "      <td>0.255408</td>\n",
              "      <td>0.431540</td>\n",
              "      <td>0.199388</td>\n",
              "      <td>0.724669</td>\n",
              "      <td>0.725010</td>\n",
              "      <td>0.485919</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.303609</td>\n",
              "      <td>-0.651067</td>\n",
              "      <td>-0.681199</td>\n",
              "      <td>-0.703426</td>\n",
              "      <td>-0.656917</td>\n",
              "      <td>0.388550</td>\n",
              "      <td>0.665633</td>\n",
              "      <td>-0.332963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fatalities</th>\n",
              "      <td>0.405847</td>\n",
              "      <td>-0.011432</td>\n",
              "      <td>0.379259</td>\n",
              "      <td>-0.019109</td>\n",
              "      <td>0.704852</td>\n",
              "      <td>0.597472</td>\n",
              "      <td>0.867637</td>\n",
              "      <td>-0.223507</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.531425</td>\n",
              "      <td>-0.303609</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.768112</td>\n",
              "      <td>0.729886</td>\n",
              "      <td>0.592716</td>\n",
              "      <td>0.432180</td>\n",
              "      <td>-0.351354</td>\n",
              "      <td>-0.321771</td>\n",
              "      <td>-0.157203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fatality_rate_permi</th>\n",
              "      <td>-0.594581</td>\n",
              "      <td>0.052281</td>\n",
              "      <td>-0.632285</td>\n",
              "      <td>0.013349</td>\n",
              "      <td>-0.241437</td>\n",
              "      <td>-0.216260</td>\n",
              "      <td>0.506594</td>\n",
              "      <td>-0.758980</td>\n",
              "      <td>-0.561814</td>\n",
              "      <td>-0.793770</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.276554</td>\n",
              "      <td>-0.651067</td>\n",
              "      <td>0.768112</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975832</td>\n",
              "      <td>0.926984</td>\n",
              "      <td>0.773831</td>\n",
              "      <td>-0.896857</td>\n",
              "      <td>-0.569843</td>\n",
              "      <td>0.360457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fatality_rate_pertruck</th>\n",
              "      <td>-0.698941</td>\n",
              "      <td>0.082117</td>\n",
              "      <td>-0.720472</td>\n",
              "      <td>0.066455</td>\n",
              "      <td>-0.284492</td>\n",
              "      <td>-0.319471</td>\n",
              "      <td>0.367880</td>\n",
              "      <td>-0.773108</td>\n",
              "      <td>-0.713140</td>\n",
              "      <td>-0.950924</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.178797</td>\n",
              "      <td>-0.681199</td>\n",
              "      <td>0.729886</td>\n",
              "      <td>0.975832</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.970407</td>\n",
              "      <td>0.881461</td>\n",
              "      <td>-0.919147</td>\n",
              "      <td>-0.695070</td>\n",
              "      <td>0.597532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vehicle_inv_permi</th>\n",
              "      <td>-0.757771</td>\n",
              "      <td>0.052475</td>\n",
              "      <td>-0.791350</td>\n",
              "      <td>0.035380</td>\n",
              "      <td>-0.420252</td>\n",
              "      <td>-0.340507</td>\n",
              "      <td>0.343080</td>\n",
              "      <td>-0.801980</td>\n",
              "      <td>-0.779739</td>\n",
              "      <td>-0.923391</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068799</td>\n",
              "      <td>-0.703426</td>\n",
              "      <td>0.592716</td>\n",
              "      <td>0.926984</td>\n",
              "      <td>0.970407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.948057</td>\n",
              "      <td>-0.972071</td>\n",
              "      <td>-0.798228</td>\n",
              "      <td>0.556885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vehicle_inv_pertruck</th>\n",
              "      <td>-0.817166</td>\n",
              "      <td>0.081672</td>\n",
              "      <td>-0.836376</td>\n",
              "      <td>0.076445</td>\n",
              "      <td>-0.425770</td>\n",
              "      <td>-0.405917</td>\n",
              "      <td>0.233576</td>\n",
              "      <td>-0.735997</td>\n",
              "      <td>-0.913235</td>\n",
              "      <td>-0.972356</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121891</td>\n",
              "      <td>-0.656917</td>\n",
              "      <td>0.432180</td>\n",
              "      <td>0.773831</td>\n",
              "      <td>0.881461</td>\n",
              "      <td>0.948057</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.942649</td>\n",
              "      <td>-0.901828</td>\n",
              "      <td>0.711926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miles_per_emp</th>\n",
              "      <td>0.376718</td>\n",
              "      <td>0.098444</td>\n",
              "      <td>0.420454</td>\n",
              "      <td>0.040685</td>\n",
              "      <td>-0.069084</td>\n",
              "      <td>0.058335</td>\n",
              "      <td>-0.243179</td>\n",
              "      <td>0.023180</td>\n",
              "      <td>0.693731</td>\n",
              "      <td>0.812604</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.308729</td>\n",
              "      <td>0.388550</td>\n",
              "      <td>-0.351354</td>\n",
              "      <td>-0.896857</td>\n",
              "      <td>-0.919147</td>\n",
              "      <td>-0.972071</td>\n",
              "      <td>-0.942649</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977969</td>\n",
              "      <td>-0.589257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miles_per_est</th>\n",
              "      <td>0.653261</td>\n",
              "      <td>0.044368</td>\n",
              "      <td>0.677214</td>\n",
              "      <td>0.047893</td>\n",
              "      <td>0.233375</td>\n",
              "      <td>0.193486</td>\n",
              "      <td>-0.334054</td>\n",
              "      <td>0.632946</td>\n",
              "      <td>0.850378</td>\n",
              "      <td>0.869147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302632</td>\n",
              "      <td>0.665633</td>\n",
              "      <td>-0.321771</td>\n",
              "      <td>-0.569843</td>\n",
              "      <td>-0.695070</td>\n",
              "      <td>-0.798228</td>\n",
              "      <td>-0.901828</td>\n",
              "      <td>0.977969</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.408253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emp_per_est</th>\n",
              "      <td>-0.859579</td>\n",
              "      <td>-0.017111</td>\n",
              "      <td>-0.861883</td>\n",
              "      <td>0.039406</td>\n",
              "      <td>-0.371789</td>\n",
              "      <td>-0.528807</td>\n",
              "      <td>-0.109878</td>\n",
              "      <td>-0.539477</td>\n",
              "      <td>-0.832893</td>\n",
              "      <td>-0.994634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.810949</td>\n",
              "      <td>-0.332963</td>\n",
              "      <td>-0.157203</td>\n",
              "      <td>0.360457</td>\n",
              "      <td>0.597532</td>\n",
              "      <td>0.556885</td>\n",
              "      <td>0.711926</td>\n",
              "      <td>-0.589257</td>\n",
              "      <td>-0.408253</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35 rows × 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9efd9d45-8d7f-4c9a-9910-b7087b848eb6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9efd9d45-8d7f-4c9a-9910-b7087b848eb6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9efd9d45-8d7f-4c9a-9910-b7087b848eb6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.loc[1987:2020].interpolate().fillna(method = 'bfill')"
      ],
      "metadata": {
        "id": "rIXoUeetz_Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select features using recursive feature elimination (RFE)\n",
        "\n",
        "X = df2.iloc[:, 1:]\n",
        "y = df2.iloc[:, 0]\n",
        "\n",
        "estimator = LinearRegression()\n",
        "selector = RFE(estimator, n_features_to_select=10)\n",
        "selector = selector.fit(X, y)\n",
        "print(X.columns[selector.support_])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9c1QpMi4Dlx",
        "outputId": "52e3ff8b-61ec-4c3d-ef03-326af69fe7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['lbr_prod_pctchg', 'freight_val_pctchg', 'freight_val_pcttotal',\n",
            "       'freight_tons_pctchg', 'freight_tons_pcttotal',\n",
            "       'freight_tonmi_pcttotal', 'cptm', 'fatality_rate_permi',\n",
            "       'fatality_rate_pertruck', 'vehicle_inv_permi'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create new dataframe with 'lbr_prod' as the target variable and (4) features. 'freight_val_pctchg','freight_tonmi_pcttotal', 'cptm', 'fatality_rate_permi' will be used to eliminate redundancy and multicollinearity.\n",
        "\n",
        "df3 = df2[['lbr_prod', 'freight_val_pctchg','freight_tonmi_pcttotal', 'cptm', 'fatality_rate_permi']].loc[1997:2018]"
      ],
      "metadata": {
        "id": "Nz9-OgSNFZgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "qxBQgfLOFk-t",
        "outputId": "211ba3f1-8076-4c16-fec4-a6fb3ab7b972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      lbr_prod  freight_val_pctchg  freight_tonmi_pcttotal    cptm  \\\n",
              "year                                                                 \n",
              "1997      86.0            0.129326                0.674276  0.0997   \n",
              "1998      86.0            0.154207                0.678996  0.1017   \n",
              "1999      86.0            0.179088                0.683716  0.1056   \n",
              "2000      88.0            0.203968                0.688436  0.1112   \n",
              "2001      89.0            0.228849                0.693157  0.1076   \n",
              "2002      92.0            0.253730                0.697877  0.1003   \n",
              "2003      95.0            0.271087                0.689827  0.1012   \n",
              "2004      94.0            0.288444                0.681777  0.1088   \n",
              "2005      95.0            0.305802                0.673728  0.1219   \n",
              "2006      95.0            0.323159                0.665678  0.1340   \n",
              "2007      96.0            0.340516                0.657629  0.1322   \n",
              "2008      96.0            0.315515                0.648807  0.1596   \n",
              "2009      90.0            0.290514                0.639985  0.1203   \n",
              "2010     101.0            0.265513                0.631163  0.1430   \n",
              "2011     103.0            0.240512                0.622342  0.1751   \n",
              "2012     100.0            0.215511                0.613520  0.1672   \n",
              "2013     101.0            0.177669                0.620705  0.1599   \n",
              "2014     105.0            0.139827                0.627889  0.1768   \n",
              "2015     106.0            0.101985                0.635074  0.1775   \n",
              "2016     105.0            0.064143                0.642258  0.1734   \n",
              "2017     111.0            0.026302                0.649443  0.1610   \n",
              "2018     112.0            0.026302                0.649443  0.1755   \n",
              "\n",
              "      fatality_rate_permi  \n",
              "year                       \n",
              "1997                 0.38  \n",
              "1998                 0.38  \n",
              "1999                 0.37  \n",
              "2000                 0.37  \n",
              "2001                 0.34  \n",
              "2002                 0.32  \n",
              "2003                 0.33  \n",
              "2004                 0.35  \n",
              "2005                 0.36  \n",
              "2006                 0.36  \n",
              "2007                 0.26  \n",
              "2008                 0.22  \n",
              "2009                 0.17  \n",
              "2010                 0.18  \n",
              "2011                 0.24  \n",
              "2012                 0.26  \n",
              "2013                 0.25  \n",
              "2014                 0.24  \n",
              "2015                 0.24  \n",
              "2016                 0.28  \n",
              "2017                 0.30  \n",
              "2018                 0.29  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8feee29-92e4-4efc-bb46-ee09eb0d14d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lbr_prod</th>\n",
              "      <th>freight_val_pctchg</th>\n",
              "      <th>freight_tonmi_pcttotal</th>\n",
              "      <th>cptm</th>\n",
              "      <th>fatality_rate_permi</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>86.0</td>\n",
              "      <td>0.129326</td>\n",
              "      <td>0.674276</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>86.0</td>\n",
              "      <td>0.154207</td>\n",
              "      <td>0.678996</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>86.0</td>\n",
              "      <td>0.179088</td>\n",
              "      <td>0.683716</td>\n",
              "      <td>0.1056</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>88.0</td>\n",
              "      <td>0.203968</td>\n",
              "      <td>0.688436</td>\n",
              "      <td>0.1112</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>89.0</td>\n",
              "      <td>0.228849</td>\n",
              "      <td>0.693157</td>\n",
              "      <td>0.1076</td>\n",
              "      <td>0.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002</th>\n",
              "      <td>92.0</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.697877</td>\n",
              "      <td>0.1003</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>95.0</td>\n",
              "      <td>0.271087</td>\n",
              "      <td>0.689827</td>\n",
              "      <td>0.1012</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>94.0</td>\n",
              "      <td>0.288444</td>\n",
              "      <td>0.681777</td>\n",
              "      <td>0.1088</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>95.0</td>\n",
              "      <td>0.305802</td>\n",
              "      <td>0.673728</td>\n",
              "      <td>0.1219</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>95.0</td>\n",
              "      <td>0.323159</td>\n",
              "      <td>0.665678</td>\n",
              "      <td>0.1340</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>96.0</td>\n",
              "      <td>0.340516</td>\n",
              "      <td>0.657629</td>\n",
              "      <td>0.1322</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008</th>\n",
              "      <td>96.0</td>\n",
              "      <td>0.315515</td>\n",
              "      <td>0.648807</td>\n",
              "      <td>0.1596</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2009</th>\n",
              "      <td>90.0</td>\n",
              "      <td>0.290514</td>\n",
              "      <td>0.639985</td>\n",
              "      <td>0.1203</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010</th>\n",
              "      <td>101.0</td>\n",
              "      <td>0.265513</td>\n",
              "      <td>0.631163</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>103.0</td>\n",
              "      <td>0.240512</td>\n",
              "      <td>0.622342</td>\n",
              "      <td>0.1751</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.215511</td>\n",
              "      <td>0.613520</td>\n",
              "      <td>0.1672</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013</th>\n",
              "      <td>101.0</td>\n",
              "      <td>0.177669</td>\n",
              "      <td>0.620705</td>\n",
              "      <td>0.1599</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014</th>\n",
              "      <td>105.0</td>\n",
              "      <td>0.139827</td>\n",
              "      <td>0.627889</td>\n",
              "      <td>0.1768</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015</th>\n",
              "      <td>106.0</td>\n",
              "      <td>0.101985</td>\n",
              "      <td>0.635074</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>105.0</td>\n",
              "      <td>0.064143</td>\n",
              "      <td>0.642258</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>111.0</td>\n",
              "      <td>0.026302</td>\n",
              "      <td>0.649443</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>112.0</td>\n",
              "      <td>0.026302</td>\n",
              "      <td>0.649443</td>\n",
              "      <td>0.1755</td>\n",
              "      <td>0.29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8feee29-92e4-4efc-bb46-ee09eb0d14d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8feee29-92e4-4efc-bb46-ee09eb0d14d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8feee29-92e4-4efc-bb46-ee09eb0d14d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLS model"
      ],
      "metadata": {
        "id": "buP57tHQ615m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df3.iloc[:, 1:]\n",
        "y = df3.iloc[:, 0]\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NajTBbp8bwd",
        "outputId": "63cacdab-26e1-46fd-93f9-418cbdfbedc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:               lbr_prod   R-squared:                       0.823\n",
            "Model:                            OLS   Adj. R-squared:                  0.781\n",
            "Method:                 Least Squares   F-statistic:                     19.73\n",
            "Date:                Wed, 15 Mar 2023   Prob (F-statistic):           3.28e-06\n",
            "Time:                        15:17:29   Log-Likelihood:                -57.154\n",
            "No. Observations:                  22   AIC:                             124.3\n",
            "Df Residuals:                      17   BIC:                             129.8\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==========================================================================================\n",
            "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------------\n",
            "const                    -13.9747     53.114     -0.263      0.796    -126.034      98.085\n",
            "freight_val_pctchg       -13.1601     10.585     -1.243      0.231     -35.492       9.172\n",
            "freight_tonmi_pcttotal   124.8554     75.551      1.653      0.117     -34.544     284.255\n",
            "cptm                     277.0064     59.025      4.693      0.000     152.474     401.538\n",
            "fatality_rate_permi      -21.8284     21.690     -1.006      0.328     -67.590      23.933\n",
            "==============================================================================\n",
            "Omnibus:                        0.628   Durbin-Watson:                   1.182\n",
            "Prob(Omnibus):                  0.730   Jarque-Bera (JB):                0.671\n",
            "Skew:                           0.183   Prob(JB):                        0.715\n",
            "Kurtosis:                       2.227   Cond. No.                         165.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEUY6CAo7xXS",
        "outputId": "b69f2075-ccf9-44c6-bffa-4b4acc30a7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.568920591615692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"df = pd.DataFrame( %s )\" % (str(df3.to_dict())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoae8Dmsw4RJ",
        "outputId": "e0e62493-be81-42ac-e86d-4bb0989f077c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df = pd.DataFrame( {'lbr_prod': {1997: 86.0, 1998: 86.0, 1999: 86.0, 2000: 88.0, 2001: 89.0, 2002: 92.0, 2003: 95.0, 2004: 94.0, 2005: 95.0, 2006: 95.0, 2007: 96.0, 2008: 96.0, 2009: 90.0, 2010: 101.0, 2011: 103.0, 2012: 100.0, 2013: 101.0, 2014: 105.0, 2015: 106.0, 2016: 105.0, 2017: 111.0, 2018: 112.0}, 'freight_val_pctchg': {1997: 0.1293260473588342, 1998: 0.15420680562900282, 1999: 0.17908756389917146, 2000: 0.20396832216934008, 2001: 0.2288490804395087, 2002: 0.2537298387096773, 2003: 0.2710871112990114, 2004: 0.28844438388834553, 2005: 0.30580165647767965, 2006: 0.32315892906701377, 2007: 0.3405162016563479, 2008: 0.31551516861874435, 2009: 0.29051413558114075, 2010: 0.2655131025435372, 2011: 0.24051206950593368, 2012: 0.2155110364683301, 2013: 0.17766913018626043, 2014: 0.13982722390419075, 2015: 0.10198531762212107, 2016: 0.06414341134005139, 2017: 0.0263015050579817, 2018: 0.0263015050579817}, 'freight_tonmi_pcttotal': {1997: 0.6742758361518226, 1998: 0.6789960035294887, 1999: 0.6837161709071548, 2000: 0.6884363382848209, 2001: 0.6931565056624869, 2002: 0.697876673040153, 2003: 0.6898270783424363, 2004: 0.6817774836447196, 2005: 0.673727888947003, 2006: 0.6656782942492863, 2007: 0.6576286995515696, 2008: 0.6488069327052287, 2009: 0.6399851658588879, 2010: 0.631163399012547, 2011: 0.6223416321662062, 2012: 0.6135198653198654, 2013: 0.6207045030996523, 2014: 0.6278891408794393, 2015: 0.6350737786592262, 2016: 0.6422584164390132, 2017: 0.6494430542188001, 2018: 0.6494430542188001}, 'cptm': {1997: 0.0997, 1998: 0.1017, 1999: 0.1056, 2000: 0.1112, 2001: 0.1076, 2002: 0.1003, 2003: 0.1012, 2004: 0.1088, 2005: 0.1219, 2006: 0.134, 2007: 0.1322, 2008: 0.1596, 2009: 0.1202999999999999, 2010: 0.143, 2011: 0.1751, 2012: 0.1672, 2013: 0.1599, 2014: 0.1767999999999999, 2015: 0.1775, 2016: 0.1734, 2017: 0.161, 2018: 0.1755}, 'fatality_rate_permi': {1997: 0.38, 1998: 0.38, 1999: 0.37, 2000: 0.37, 2001: 0.34, 2002: 0.32, 2003: 0.33, 2004: 0.35, 2005: 0.36, 2006: 0.36, 2007: 0.26, 2008: 0.22, 2009: 0.17, 2010: 0.18, 2011: 0.24, 2012: 0.26, 2013: 0.25, 2014: 0.24, 2015: 0.24, 2016: 0.28, 2017: 0.3, 2018: 0.29}} )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lasso"
      ],
      "metadata": {
        "id": "kbuQeOnJNZaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LASSO trial 1"
      ],
      "metadata": {
        "id": "hwbaV90C7te8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df3.iloc[:, 1:]\n",
        "y = df3.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "lasso_model = LassoCV(cv=5, random_state=0)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "n = len(y_test)\n",
        "p = X.shape[1]\n",
        "adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))\n",
        "\n",
        "print(\"R-squared:\", r_squared)\n",
        "print(\"Adjusted R-squared:\", adjusted_r_squared)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(lasso_model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA5_H8QPzRwr",
        "outputId": "c81da1bf-9e6d-4bc9-e655-2d8904cc993b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: 0.46686773894962075\n",
            "Adjusted R-squared: -inf\n",
            "Mean squared error: 22.732759611188172\n",
            "[-2.12837376 -0.          5.01421888  0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-02019b077006>:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LASSO trial 2"
      ],
      "metadata": {
        "id": "6pgoVSu27wp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_model = LassoCV(cv=5, random_state=0, max_iter=10000, alphas=[0.001, 0.01, 0.1, 1, 10, 100])\n",
        "lasso_model.fit(X_train, y_train)\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "adj_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
        "print(\"R-squared:\", r_squared)\n",
        "print(\"Adjusted R-squared:\", adjusted_r_squared)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(lasso_model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm32Nd7pjyDz",
        "outputId": "e71d6a3d-73b7-4666-f738-5fa6329acd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: 0.4604425580600703\n",
            "Adjusted R-squared: -inf\n",
            "Mean squared error: 23.006729324318602\n",
            "[-1.81513638 -0.          4.70100949 -0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-e4cb4e1fab84>:6: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adj_r_squared = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network"
      ],
      "metadata": {
        "id": "dXGQweR764ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NN Trial 1"
      ],
      "metadata": {
        "id": "EeeZ-SIt7bsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df3.iloc[:, 1:], df3.iloc[:, 0], test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the input variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=4, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse, mae = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "n = X_test_scaled.shape[0]  # Number of samples\n",
        "p = X_test_scaled.shape[1]  # Number of features\n",
        "adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print('R-squared:', r2)\n",
        "print('Adjusted R-squared:', adj_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upt8XazGRGW1",
        "outputId": "dcf32b3d-2726-4e55-c0e6-c5daa7f986df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9455.2432 - mae: 96.8775 - val_loss: 9862.8467 - val_mae: 99.2236\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 9443.3818 - mae: 96.8160 - val_loss: 9850.6650 - val_mae: 99.1623\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 9431.2461 - mae: 96.7530 - val_loss: 9838.6602 - val_mae: 99.1019\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9418.9062 - mae: 96.6889 - val_loss: 9826.5830 - val_mae: 99.0411\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 9406.4805 - mae: 96.6242 - val_loss: 9814.2471 - val_mae: 98.9789\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 9394.0098 - mae: 96.5593 - val_loss: 9801.6562 - val_mae: 98.9154\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 9381.3750 - mae: 96.4936 - val_loss: 9788.7598 - val_mae: 98.8503\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9368.5918 - mae: 96.4270 - val_loss: 9775.4844 - val_mae: 98.7833\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 9355.5303 - mae: 96.3591 - val_loss: 9761.8789 - val_mae: 98.7146\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9342.1631 - mae: 96.2894 - val_loss: 9748.1621 - val_mae: 98.6452\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9328.5449 - mae: 96.2184 - val_loss: 9734.3027 - val_mae: 98.5750\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 9314.7432 - mae: 96.1462 - val_loss: 9720.1787 - val_mae: 98.5035\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9300.6914 - mae: 96.0727 - val_loss: 9705.9199 - val_mae: 98.4313\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 9286.4121 - mae: 95.9980 - val_loss: 9691.2705 - val_mae: 98.3570\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 9271.9219 - mae: 95.9221 - val_loss: 9676.1904 - val_mae: 98.2805\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 9257.2910 - mae: 95.8456 - val_loss: 9660.8643 - val_mae: 98.2027\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9242.3965 - mae: 95.7675 - val_loss: 9645.4180 - val_mae: 98.1242\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 9227.1943 - mae: 95.6879 - val_loss: 9629.6289 - val_mae: 98.0439\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 9211.7188 - mae: 95.6067 - val_loss: 9613.5107 - val_mae: 97.9619\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9195.9355 - mae: 95.5240 - val_loss: 9597.0879 - val_mae: 97.8782\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9179.8447 - mae: 95.4396 - val_loss: 9580.1719 - val_mae: 97.7920\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 9163.4756 - mae: 95.3537 - val_loss: 9562.7861 - val_mae: 97.7033\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 9146.6602 - mae: 95.2655 - val_loss: 9545.0527 - val_mae: 97.6127\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 9129.4932 - mae: 95.1753 - val_loss: 9527.0020 - val_mae: 97.5205\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 9111.8887 - mae: 95.0827 - val_loss: 9508.5947 - val_mae: 97.4263\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 9093.8682 - mae: 94.9879 - val_loss: 9489.7812 - val_mae: 97.3299\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 9075.3711 - mae: 94.8905 - val_loss: 9470.4941 - val_mae: 97.2310\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 9056.4150 - mae: 94.7906 - val_loss: 9450.7100 - val_mae: 97.1295\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 9037.0674 - mae: 94.6886 - val_loss: 9430.5000 - val_mae: 97.0257\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9017.3008 - mae: 94.5842 - val_loss: 9409.7832 - val_mae: 96.9193\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 8996.9258 - mae: 94.4765 - val_loss: 9388.1680 - val_mae: 96.8082\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 8975.7754 - mae: 94.3647 - val_loss: 9365.8867 - val_mae: 96.6935\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 8953.9824 - mae: 94.2494 - val_loss: 9343.1270 - val_mae: 96.5763\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 8931.5859 - mae: 94.1308 - val_loss: 9319.8477 - val_mae: 96.4562\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8908.6318 - mae: 94.0092 - val_loss: 9295.9854 - val_mae: 96.3329\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 8885.0234 - mae: 93.8840 - val_loss: 9271.3037 - val_mae: 96.2052\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 8860.7852 - mae: 93.7553 - val_loss: 9246.0693 - val_mae: 96.0745\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 8835.9941 - mae: 93.6236 - val_loss: 9220.2646 - val_mae: 95.9406\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 8810.6318 - mae: 93.4887 - val_loss: 9193.8809 - val_mae: 95.8035\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 8784.5469 - mae: 93.3497 - val_loss: 9166.9238 - val_mae: 95.6633\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 8757.6523 - mae: 93.2061 - val_loss: 9139.2422 - val_mae: 95.5191\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8730.0400 - mae: 93.0585 - val_loss: 9110.8828 - val_mae: 95.3712\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 8701.6602 - mae: 92.9066 - val_loss: 9081.8135 - val_mae: 95.2194\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 8672.5938 - mae: 92.7507 - val_loss: 9052.0791 - val_mae: 95.0638\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 8642.6836 - mae: 92.5899 - val_loss: 9021.6328 - val_mae: 94.9041\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 8611.9941 - mae: 92.4246 - val_loss: 8990.4990 - val_mae: 94.7406\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 8580.6172 - mae: 92.2552 - val_loss: 8958.6621 - val_mae: 94.5731\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 8548.5332 - mae: 92.0818 - val_loss: 8926.0967 - val_mae: 94.4015\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 8515.7617 - mae: 91.9043 - val_loss: 8892.8203 - val_mae: 94.2257\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 8482.2637 - mae: 91.7225 - val_loss: 8858.8105 - val_mae: 94.0458\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 8448.0459 - mae: 91.5364 - val_loss: 8824.0781 - val_mae: 93.8616\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 8413.0898 - mae: 91.3459 - val_loss: 8788.6182 - val_mae: 93.6731\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 8377.3730 - mae: 91.1508 - val_loss: 8752.4160 - val_mae: 93.4804\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 8340.8975 - mae: 90.9512 - val_loss: 8715.4639 - val_mae: 93.2831\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 8303.6680 - mae: 90.7469 - val_loss: 8677.7598 - val_mae: 93.0815\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 8265.6768 - mae: 90.5380 - val_loss: 8639.2969 - val_mae: 92.8752\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 8226.9053 - mae: 90.3243 - val_loss: 8600.0654 - val_mae: 92.6644\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 8187.3281 - mae: 90.1057 - val_loss: 8560.0557 - val_mae: 92.4488\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 8146.9219 - mae: 89.8820 - val_loss: 8519.2227 - val_mae: 92.2283\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 8105.7080 - mae: 89.6532 - val_loss: 8477.5996 - val_mae: 92.0029\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 8063.5986 - mae: 89.4188 - val_loss: 8435.1768 - val_mae: 91.7726\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 8020.6230 - mae: 89.1790 - val_loss: 8391.9443 - val_mae: 91.5372\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 7976.7344 - mae: 88.9335 - val_loss: 8347.8975 - val_mae: 91.2968\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 7932.0151 - mae: 88.6827 - val_loss: 8302.8633 - val_mae: 91.0503\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 7886.4219 - mae: 88.4262 - val_loss: 8256.8398 - val_mae: 90.7977\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 7840.0073 - mae: 88.1644 - val_loss: 8210.0068 - val_mae: 90.5399\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 7792.7500 - mae: 87.8970 - val_loss: 8162.3125 - val_mae: 90.2765\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 7744.5947 - mae: 87.6236 - val_loss: 8113.7783 - val_mae: 90.0077\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 7695.5034 - mae: 87.3441 - val_loss: 8064.3804 - val_mae: 89.7332\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 7645.5098 - mae: 87.0584 - val_loss: 8014.1211 - val_mae: 89.4530\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 7594.5723 - mae: 86.7664 - val_loss: 7962.9951 - val_mae: 89.1670\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 7542.6489 - mae: 86.4677 - val_loss: 7910.9858 - val_mae: 88.8750\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 7489.7319 - mae: 86.1621 - val_loss: 7858.0845 - val_mae: 88.5770\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 7435.8940 - mae: 85.8502 - val_loss: 7804.2900 - val_mae: 88.2728\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 7380.9951 - mae: 85.5309 - val_loss: 7749.3794 - val_mae: 87.9612\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 7325.1221 - mae: 85.2048 - val_loss: 7693.5449 - val_mae: 87.6431\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 7268.2993 - mae: 84.8718 - val_loss: 7636.7744 - val_mae: 87.3185\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 7210.4878 - mae: 84.5317 - val_loss: 7578.7002 - val_mae: 86.9851\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 7151.6274 - mae: 84.1840 - val_loss: 7519.5356 - val_mae: 86.6440\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 7091.7285 - mae: 83.8286 - val_loss: 7459.3853 - val_mae: 86.2957\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 7030.8560 - mae: 83.4659 - val_loss: 7398.2295 - val_mae: 85.9400\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 6969.0166 - mae: 83.0958 - val_loss: 7336.1616 - val_mae: 85.5775\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 6906.1826 - mae: 82.7180 - val_loss: 7273.1909 - val_mae: 85.2079\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 6842.3511 - mae: 82.3325 - val_loss: 7209.3247 - val_mae: 84.8313\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 6777.6045 - mae: 81.9395 - val_loss: 7144.5752 - val_mae: 84.4477\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 6711.9541 - mae: 81.5390 - val_loss: 7078.9521 - val_mae: 84.0570\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6645.3545 - mae: 81.1306 - val_loss: 7012.4629 - val_mae: 83.6591\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 6577.7837 - mae: 80.7141 - val_loss: 6945.1201 - val_mae: 83.2540\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 6509.3364 - mae: 80.2899 - val_loss: 6876.9360 - val_mae: 82.8417\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 6440.0107 - mae: 79.8580 - val_loss: 6807.9229 - val_mae: 82.4221\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 6369.8301 - mae: 79.4182 - val_loss: 6738.0942 - val_mae: 81.9952\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 6298.8081 - mae: 78.9706 - val_loss: 6667.4614 - val_mae: 81.5610\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 6226.9634 - mae: 78.5152 - val_loss: 6596.0239 - val_mae: 81.1192\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 6154.3096 - mae: 78.0519 - val_loss: 6523.8105 - val_mae: 80.6700\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 6080.8618 - mae: 77.5807 - val_loss: 6450.7383 - val_mae: 80.2126\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 6006.6396 - mae: 77.1014 - val_loss: 6376.9229 - val_mae: 79.7478\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 5931.6621 - mae: 76.6142 - val_loss: 6302.3867 - val_mae: 79.2753\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 5855.9019 - mae: 76.1187 - val_loss: 6227.0029 - val_mae: 78.7945\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 5779.3730 - mae: 75.6148 - val_loss: 6150.9199 - val_mae: 78.3059\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 5702.1382 - mae: 75.1027 - val_loss: 6074.1646 - val_mae: 77.8097\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 4980.2373 - mae: 70.2907\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "Mean Squared Error: 4980.2373046875\n",
            "Mean Absolute Error: 70.29066467285156\n",
            "R-squared: -115.79730251656841\n",
            "Adjusted R-squared: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1bbff5f676ac>:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NN Trial 2"
      ],
      "metadata": {
        "id": "utUPcBov7e4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=4, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate = 0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=500, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse, mae = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "n = X_test_scaled.shape[0]  # Number of samples\n",
        "p = X_test_scaled.shape[1]  # Number of features\n",
        "adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print('R-squared:', r2)\n",
        "print('Adjusted R-squared:', adj_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcrR-DXjduc4",
        "outputId": "8b21602f-3164-49a7-a9cc-a106f0c36220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9578.5645 - mae: 97.4962 - val_loss: 9992.1025 - val_mae: 99.8650\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 9571.2363 - mae: 97.4609 - val_loss: 9976.7949 - val_mae: 99.7892\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9562.4180 - mae: 97.4122 - val_loss: 9961.9902 - val_mae: 99.7158\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9523.1416 - mae: 97.2184 - val_loss: 9947.7207 - val_mae: 99.6449\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9522.0283 - mae: 97.2145 - val_loss: 9933.8760 - val_mae: 99.5760\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9503.0254 - mae: 97.1108 - val_loss: 9920.1445 - val_mae: 99.5076\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9485.0176 - mae: 97.0236 - val_loss: 9906.3906 - val_mae: 99.4390\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9486.0664 - mae: 97.0349 - val_loss: 9892.6953 - val_mae: 99.3707\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9462.0596 - mae: 96.9077 - val_loss: 9878.9814 - val_mae: 99.3021\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 9464.5225 - mae: 96.9237 - val_loss: 9865.1230 - val_mae: 99.2328\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9430.4980 - mae: 96.7468 - val_loss: 9851.1797 - val_mae: 99.1630\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9427.6807 - mae: 96.7353 - val_loss: 9837.4785 - val_mae: 99.0943\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9406.1436 - mae: 96.6213 - val_loss: 9823.5527 - val_mae: 99.0246\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9397.2646 - mae: 96.5768 - val_loss: 9809.5322 - val_mae: 98.9543\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9401.6455 - mae: 96.5992 - val_loss: 9795.3281 - val_mae: 98.8831\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9370.3877 - mae: 96.4492 - val_loss: 9780.4307 - val_mae: 98.8083\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9344.1318 - mae: 96.3087 - val_loss: 9765.0596 - val_mae: 98.7311\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9340.7812 - mae: 96.2934 - val_loss: 9749.1152 - val_mae: 98.6511\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9305.3428 - mae: 96.1027 - val_loss: 9732.5420 - val_mae: 98.5677\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9284.3174 - mae: 95.9944 - val_loss: 9715.7070 - val_mae: 98.4831\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9274.3271 - mae: 95.9490 - val_loss: 9698.3359 - val_mae: 98.3956\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9272.8193 - mae: 95.9396 - val_loss: 9680.2236 - val_mae: 98.3044\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9262.0107 - mae: 95.8875 - val_loss: 9661.3760 - val_mae: 98.2093\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9239.2158 - mae: 95.7671 - val_loss: 9641.9609 - val_mae: 98.1113\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9212.7715 - mae: 95.6229 - val_loss: 9621.8408 - val_mae: 98.0095\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9194.4922 - mae: 95.5420 - val_loss: 9600.8525 - val_mae: 97.9033\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9144.1426 - mae: 95.2929 - val_loss: 9579.0146 - val_mae: 97.7926\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9182.4824 - mae: 95.4900 - val_loss: 9556.5205 - val_mae: 97.6784\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9130.8115 - mae: 95.1937 - val_loss: 9533.2285 - val_mae: 97.5600\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9140.8555 - mae: 95.2738 - val_loss: 9509.1797 - val_mae: 97.4377\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9055.4951 - mae: 94.8167 - val_loss: 9484.1055 - val_mae: 97.3099\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9025.4062 - mae: 94.6480 - val_loss: 9458.0938 - val_mae: 97.1772\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9030.7441 - mae: 94.6846 - val_loss: 9431.0879 - val_mae: 97.0391\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8959.1123 - mae: 94.3202 - val_loss: 9403.0742 - val_mae: 96.8957\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8964.6807 - mae: 94.3408 - val_loss: 9373.8574 - val_mae: 96.7459\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8921.0391 - mae: 94.1254 - val_loss: 9343.6182 - val_mae: 96.5906\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 8883.8477 - mae: 93.9272 - val_loss: 9312.2939 - val_mae: 96.4294\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8835.0498 - mae: 93.6658 - val_loss: 9279.7256 - val_mae: 96.2615\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 8862.0771 - mae: 93.8035 - val_loss: 9246.0439 - val_mae: 96.0875\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8831.4629 - mae: 93.6515 - val_loss: 9211.3574 - val_mae: 95.9080\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8777.9385 - mae: 93.3656 - val_loss: 9175.5527 - val_mae: 95.7224\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8756.6152 - mae: 93.2579 - val_loss: 9138.5820 - val_mae: 95.5303\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8674.0186 - mae: 92.8248 - val_loss: 9100.3486 - val_mae: 95.3312\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8671.7314 - mae: 92.7988 - val_loss: 9060.7568 - val_mae: 95.1246\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8665.2812 - mae: 92.7765 - val_loss: 9020.0195 - val_mae: 94.9115\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8542.9717 - mae: 92.1120 - val_loss: 8977.7227 - val_mae: 94.6898\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8506.6123 - mae: 91.9503 - val_loss: 8933.7188 - val_mae: 94.4586\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8531.3574 - mae: 92.0365 - val_loss: 8888.3672 - val_mae: 94.2197\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8475.4834 - mae: 91.7577 - val_loss: 8841.6055 - val_mae: 93.9727\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8376.5605 - mae: 91.2315 - val_loss: 8793.1895 - val_mae: 93.7162\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8424.1250 - mae: 91.4626 - val_loss: 8743.3125 - val_mae: 93.4513\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8315.9775 - mae: 90.8957 - val_loss: 8691.8604 - val_mae: 93.1771\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8209.9688 - mae: 90.3231 - val_loss: 8638.6416 - val_mae: 92.8927\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8194.4424 - mae: 90.1866 - val_loss: 8583.8467 - val_mae: 92.5988\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8176.7764 - mae: 90.1702 - val_loss: 8527.1992 - val_mae: 92.2940\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8132.9150 - mae: 89.8312 - val_loss: 8468.7764 - val_mae: 91.9785\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7979.5049 - mae: 89.0457 - val_loss: 8408.5312 - val_mae: 91.6520\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 8001.3369 - mae: 89.1823 - val_loss: 8346.6934 - val_mae: 91.3156\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7851.9990 - mae: 88.3538 - val_loss: 8282.9551 - val_mae: 90.9674\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7830.2607 - mae: 88.2289 - val_loss: 8217.4189 - val_mae: 90.6080\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7748.0361 - mae: 87.7240 - val_loss: 8150.0645 - val_mae: 90.2370\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7580.8403 - mae: 86.8246 - val_loss: 8080.4438 - val_mae: 89.8518\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7623.0601 - mae: 87.0131 - val_loss: 8009.0244 - val_mae: 89.4549\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7581.3247 - mae: 86.7421 - val_loss: 7935.7598 - val_mae: 89.0457\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7402.1323 - mae: 85.7672 - val_loss: 7860.4956 - val_mae: 88.6232\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7350.3364 - mae: 85.4706 - val_loss: 7783.1860 - val_mae: 88.1870\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 7275.6060 - mae: 84.9993 - val_loss: 7704.0029 - val_mae: 87.7379\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7147.7354 - mae: 84.3244 - val_loss: 7622.8857 - val_mae: 87.2751\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7100.3569 - mae: 84.0316 - val_loss: 7539.8110 - val_mae: 86.7985\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 7211.4775 - mae: 84.6084 - val_loss: 7455.3638 - val_mae: 86.3110\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6928.5410 - mae: 83.0102 - val_loss: 7368.7930 - val_mae: 85.8083\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6845.3838 - mae: 82.5432 - val_loss: 7280.1655 - val_mae: 85.2903\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6761.5410 - mae: 82.0218 - val_loss: 7189.4023 - val_mae: 84.7563\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6710.5210 - mae: 81.6812 - val_loss: 7096.8452 - val_mae: 84.2080\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6563.4072 - mae: 80.8078 - val_loss: 7002.4253 - val_mae: 83.6446\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6474.0630 - mae: 80.2632 - val_loss: 6906.1606 - val_mae: 83.0660\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6517.1465 - mae: 80.4871 - val_loss: 6808.4316 - val_mae: 82.4740\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6256.3174 - mae: 78.9044 - val_loss: 6708.8081 - val_mae: 81.8659\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6327.8076 - mae: 79.3283 - val_loss: 6607.8149 - val_mae: 81.2442\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6248.3315 - mae: 78.7101 - val_loss: 6505.4712 - val_mae: 80.6090\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6040.0200 - mae: 77.4560 - val_loss: 6401.6724 - val_mae: 79.9591\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 5974.3628 - mae: 76.9239 - val_loss: 6296.3647 - val_mae: 79.2938\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5817.1753 - mae: 76.0098 - val_loss: 6189.5352 - val_mae: 78.6126\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5835.5928 - mae: 76.1378 - val_loss: 6081.3755 - val_mae: 77.9163\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5327.5928 - mae: 72.7905 - val_loss: 5971.3706 - val_mae: 77.2010\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5405.3940 - mae: 73.2488 - val_loss: 5860.0571 - val_mae: 76.4698\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5323.1377 - mae: 72.7529 - val_loss: 5747.3604 - val_mae: 75.7216\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5244.8447 - mae: 72.1877 - val_loss: 5633.5176 - val_mae: 74.9574\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 5082.8428 - mae: 70.9237 - val_loss: 5518.6201 - val_mae: 74.1773\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 5289.3125 - mae: 72.4591 - val_loss: 5403.1235 - val_mae: 73.3840\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5085.4409 - mae: 71.0014 - val_loss: 5286.9175 - val_mae: 72.5763\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 4995.7529 - mae: 70.4265 - val_loss: 5169.9976 - val_mae: 71.7534\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 4951.6660 - mae: 70.1131 - val_loss: 5052.5977 - val_mae: 70.9166\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 4863.2896 - mae: 69.4724 - val_loss: 4934.8765 - val_mae: 70.0664\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4560.0669 - mae: 67.2602 - val_loss: 4816.6460 - val_mae: 69.2009\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4292.0288 - mae: 65.2184 - val_loss: 4697.8877 - val_mae: 68.3192\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4040.3303 - mae: 63.2709 - val_loss: 4578.4448 - val_mae: 67.4195\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4193.0723 - mae: 64.2503 - val_loss: 4458.8955 - val_mae: 66.5054\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 3937.1450 - mae: 62.3682 - val_loss: 4339.0942 - val_mae: 65.5753\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3954.8420 - mae: 62.4930 - val_loss: 4219.3193 - val_mae: 64.6304\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 3967.3044 - mae: 62.3518 - val_loss: 4099.8604 - val_mae: 63.6725\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3459.2502 - mae: 58.2567 - val_loss: 3980.4326 - val_mae: 62.6984\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3312.3599 - mae: 56.9035 - val_loss: 3861.1125 - val_mae: 61.7081\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 3463.8589 - mae: 58.4865 - val_loss: 3742.3391 - val_mae: 60.7042\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3268.7388 - mae: 56.6826 - val_loss: 3624.3008 - val_mae: 59.6876\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3285.9783 - mae: 56.5380 - val_loss: 3507.1531 - val_mae: 58.6592\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3007.0486 - mae: 54.1090 - val_loss: 3390.8892 - val_mae: 57.6180\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2872.6887 - mae: 52.8548 - val_loss: 3275.7251 - val_mae: 56.5653\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2793.3530 - mae: 52.1131 - val_loss: 3161.6243 - val_mae: 55.5001\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2579.7283 - mae: 50.1958 - val_loss: 3049.0076 - val_mae: 54.4255\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2713.1870 - mae: 51.3918 - val_loss: 2937.8140 - val_mae: 53.3405\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2286.4297 - mae: 46.4984 - val_loss: 2828.4985 - val_mae: 52.2490\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2528.7786 - mae: 49.4556 - val_loss: 2720.8167 - val_mae: 51.1475\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2055.0688 - mae: 44.5862 - val_loss: 2615.0469 - val_mae: 50.0388\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1962.4705 - mae: 42.5129 - val_loss: 2511.6404 - val_mae: 48.9274\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2138.3323 - mae: 45.7442 - val_loss: 2410.0142 - val_mae: 47.8061\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1895.6509 - mae: 42.1816 - val_loss: 2310.7690 - val_mae: 46.6815\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1994.5670 - mae: 43.6334 - val_loss: 2213.6780 - val_mae: 45.5501\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1769.5090 - mae: 40.5758 - val_loss: 2119.1213 - val_mae: 44.4169\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1593.6521 - mae: 38.2961 - val_loss: 2027.2233 - val_mae: 43.2825\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1623.3179 - mae: 39.0210 - val_loss: 1937.8593 - val_mae: 42.1457\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1677.9830 - mae: 39.2902 - val_loss: 1851.2091 - val_mae: 41.0085\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1312.9797 - mae: 34.3435 - val_loss: 1767.5518 - val_mae: 39.8754\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1287.3296 - mae: 34.5369 - val_loss: 1686.7170 - val_mae: 38.7436\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1527.1526 - mae: 37.0531 - val_loss: 1608.6836 - val_mae: 37.6130\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1328.6947 - mae: 34.1324 - val_loss: 1533.5936 - val_mae: 36.4862\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1095.4979 - mae: 30.4378 - val_loss: 1461.6390 - val_mae: 35.3666\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1046.9247 - mae: 30.6435 - val_loss: 1392.5476 - val_mae: 34.2497\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 960.1945 - mae: 28.5695 - val_loss: 1327.2374 - val_mae: 33.1538\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 845.8069 - mae: 26.1059 - val_loss: 1265.1459 - val_mae: 32.0724\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 970.5958 - mae: 27.1112 - val_loss: 1206.0759 - val_mae: 31.0037\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 948.9791 - mae: 27.6069 - val_loss: 1149.9050 - val_mae: 29.9477\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 629.7397 - mae: 21.4628 - val_loss: 1096.9574 - val_mae: 28.9122\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 863.1581 - mae: 25.7990 - val_loss: 1046.7059 - val_mae: 27.8878\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 679.2596 - mae: 22.0869 - val_loss: 999.6848 - val_mae: 26.8904\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 661.0846 - mae: 22.6367 - val_loss: 955.1534 - val_mae: 25.9010\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 658.9819 - mae: 20.2061 - val_loss: 913.5443 - val_mae: 24.9350\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 481.9610 - mae: 19.6280 - val_loss: 874.5561 - val_mae: 23.9888\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 463.8409 - mae: 18.8058 - val_loss: 838.2437 - val_mae: 23.0669\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 420.1899 - mae: 17.6656 - val_loss: 804.3871 - val_mae: 22.1692\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 571.3976 - mae: 19.7037 - val_loss: 772.4899 - val_mae: 21.2787\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 378.5616 - mae: 16.6078 - val_loss: 742.7001 - val_mae: 20.4015\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 471.3069 - mae: 19.5628 - val_loss: 714.6890 - val_mae: 19.5333\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 399.4340 - mae: 16.8180 - val_loss: 688.6127 - val_mae: 18.6771\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 516.3000 - mae: 18.7241 - val_loss: 664.6561 - val_mae: 17.8575\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 337.0609 - mae: 16.7528 - val_loss: 642.4577 - val_mae: 17.0606\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 611.8171 - mae: 20.7158 - val_loss: 622.4161 - val_mae: 16.3125\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 465.8233 - mae: 18.4791 - val_loss: 603.7890 - val_mae: 15.5857\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 375.4137 - mae: 13.8979 - val_loss: 586.9039 - val_mae: 14.8832\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 363.9960 - mae: 15.8056 - val_loss: 571.5316 - val_mae: 14.2220\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 290.0405 - mae: 13.3986 - val_loss: 557.5301 - val_mae: 13.7645\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 360.4589 - mae: 14.7893 - val_loss: 545.2449 - val_mae: 13.5069\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 283.5694 - mae: 14.3629 - val_loss: 534.5250 - val_mae: 13.2760\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 332.5779 - mae: 15.2685 - val_loss: 524.2318 - val_mae: 13.0475\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 294.1164 - mae: 13.6736 - val_loss: 514.6434 - val_mae: 12.8299\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 345.3116 - mae: 15.9638 - val_loss: 505.7493 - val_mae: 12.6497\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 294.4579 - mae: 14.3497 - val_loss: 497.5639 - val_mae: 12.5614\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 338.7828 - mae: 15.3162 - val_loss: 490.1890 - val_mae: 12.4736\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 382.1704 - mae: 13.2997 - val_loss: 483.0213 - val_mae: 12.3719\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 272.6668 - mae: 13.8983 - val_loss: 476.1123 - val_mae: 12.2632\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 271.3325 - mae: 14.3639 - val_loss: 469.7746 - val_mae: 12.1724\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 169.9214 - mae: 10.3609 - val_loss: 463.7084 - val_mae: 12.0876\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 239.4073 - mae: 13.7316 - val_loss: 457.9145 - val_mae: 12.0206\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 356.0455 - mae: 14.6021 - val_loss: 452.2903 - val_mae: 12.0955\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 365.4982 - mae: 15.9724 - val_loss: 446.7287 - val_mae: 12.1557\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 359.9049 - mae: 15.3415 - val_loss: 441.5355 - val_mae: 12.1883\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 323.2899 - mae: 14.3475 - val_loss: 436.1903 - val_mae: 12.1848\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 230.5389 - mae: 12.5604 - val_loss: 430.9311 - val_mae: 12.1930\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 185.9160 - mae: 11.2993 - val_loss: 425.8905 - val_mae: 12.1752\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 292.8258 - mae: 14.5925 - val_loss: 421.1003 - val_mae: 12.1456\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 305.3385 - mae: 14.5854 - val_loss: 416.7194 - val_mae: 12.0701\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 171.8179 - mae: 10.4629 - val_loss: 412.5365 - val_mae: 11.9861\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 172.6709 - mae: 9.5489 - val_loss: 408.5704 - val_mae: 11.9001\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 224.0182 - mae: 13.0795 - val_loss: 404.7704 - val_mae: 11.7860\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 258.6806 - mae: 12.3055 - val_loss: 400.8932 - val_mae: 11.7842\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 313.4451 - mae: 13.4795 - val_loss: 397.3210 - val_mae: 11.7747\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 295.8295 - mae: 12.8722 - val_loss: 393.7983 - val_mae: 11.7677\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 345.7041 - mae: 14.0728 - val_loss: 390.3291 - val_mae: 11.7595\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 307.7748 - mae: 13.7645 - val_loss: 386.8788 - val_mae: 11.7455\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 348.9697 - mae: 16.0464 - val_loss: 383.3814 - val_mae: 11.7373\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 164.3923 - mae: 11.2130 - val_loss: 380.0999 - val_mae: 11.7248\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 233.6040 - mae: 12.7746 - val_loss: 376.8942 - val_mae: 11.7052\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 181.0987 - mae: 10.8580 - val_loss: 373.6877 - val_mae: 11.6816\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 235.9796 - mae: 12.3213 - val_loss: 370.6574 - val_mae: 11.6533\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 180.6216 - mae: 10.3341 - val_loss: 367.9243 - val_mae: 11.6243\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 274.0856 - mae: 12.2542 - val_loss: 365.4294 - val_mae: 11.5909\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 373.9821 - mae: 14.5955 - val_loss: 363.2467 - val_mae: 11.5509\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 390.4782 - mae: 15.5914 - val_loss: 360.8695 - val_mae: 11.5137\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 362.9641 - mae: 15.4039 - val_loss: 358.5069 - val_mae: 11.4728\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 254.1040 - mae: 11.9149 - val_loss: 356.2665 - val_mae: 11.4283\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 275.4403 - mae: 11.8065 - val_loss: 354.1381 - val_mae: 11.3812\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 294.2312 - mae: 14.5843 - val_loss: 351.7964 - val_mae: 11.3402\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 163.7359 - mae: 11.1247 - val_loss: 349.2876 - val_mae: 11.2984\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 419.0096 - mae: 17.3342 - val_loss: 346.6086 - val_mae: 11.2658\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 188.6185 - mae: 10.9891 - val_loss: 344.1204 - val_mae: 11.2305\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 360.7083 - mae: 14.9960 - val_loss: 342.1179 - val_mae: 11.1898\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 420.8741 - mae: 17.6282 - val_loss: 340.1169 - val_mae: 11.1430\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 412.8896 - mae: 15.1353 - val_loss: 338.3336 - val_mae: 11.0932\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 278.9913 - mae: 13.8145 - val_loss: 337.4495 - val_mae: 11.0302\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 285.0536 - mae: 12.7565 - val_loss: 336.5571 - val_mae: 10.9685\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 335.7319 - mae: 13.9512 - val_loss: 335.3604 - val_mae: 10.9122\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 274.4543 - mae: 12.3970 - val_loss: 334.0794 - val_mae: 10.8504\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 126.9044 - mae: 8.9145 - val_loss: 332.4492 - val_mae: 10.7940\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 295.5302 - mae: 14.4559 - val_loss: 330.7206 - val_mae: 10.7383\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 227.8263 - mae: 12.5326 - val_loss: 329.3952 - val_mae: 10.6796\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 252.3707 - mae: 12.4938 - val_loss: 328.4661 - val_mae: 10.6203\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 209.5295 - mae: 11.6880 - val_loss: 327.2116 - val_mae: 10.5697\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 165.0701 - mae: 10.7754 - val_loss: 326.0250 - val_mae: 10.5240\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 176.1913 - mae: 9.6497 - val_loss: 324.8059 - val_mae: 10.5232\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 161.5108 - mae: 10.4866 - val_loss: 323.5938 - val_mae: 10.5263\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 307.3300 - mae: 14.9448 - val_loss: 322.8498 - val_mae: 10.5310\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 201.6062 - mae: 11.3570 - val_loss: 321.8361 - val_mae: 10.5283\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 117.7893 - mae: 8.5672 - val_loss: 320.7728 - val_mae: 10.5537\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 202.9250 - mae: 11.4981 - val_loss: 319.1112 - val_mae: 10.5632\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 237.2502 - mae: 12.6586 - val_loss: 317.2830 - val_mae: 10.5526\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 163.0922 - mae: 10.3535 - val_loss: 315.1068 - val_mae: 10.5075\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 269.4188 - mae: 13.9485 - val_loss: 312.5826 - val_mae: 10.4420\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 146.2927 - mae: 10.3322 - val_loss: 310.1822 - val_mae: 10.3885\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 224.9028 - mae: 11.9089 - val_loss: 307.3195 - val_mae: 10.3088\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 146.7392 - mae: 9.9886 - val_loss: 304.5360 - val_mae: 10.2239\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 203.2837 - mae: 10.6019 - val_loss: 301.5095 - val_mae: 10.1136\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 256.0956 - mae: 15.1062 - val_loss: 298.5373 - val_mae: 10.0223\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 365.2786 - mae: 15.7216 - val_loss: 295.4061 - val_mae: 9.9529\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 219.6882 - mae: 12.6484 - val_loss: 292.4487 - val_mae: 9.9326\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 93.6455 - mae: 7.5033 - val_loss: 289.7019 - val_mae: 9.9165\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 255.7891 - mae: 13.0894 - val_loss: 287.2974 - val_mae: 9.8890\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 345.0477 - mae: 15.5909 - val_loss: 285.2453 - val_mae: 9.8470\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 201.4313 - mae: 11.8178 - val_loss: 283.4915 - val_mae: 9.8048\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 186.6061 - mae: 11.0043 - val_loss: 281.4956 - val_mae: 9.7678\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 318.0836 - mae: 13.9492 - val_loss: 278.9856 - val_mae: 9.7340\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 234.2466 - mae: 11.1967 - val_loss: 277.0818 - val_mae: 9.6987\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 283.7024 - mae: 13.3088 - val_loss: 275.4889 - val_mae: 9.7100\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 242.5096 - mae: 12.4249 - val_loss: 273.5527 - val_mae: 9.7384\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 160.7750 - mae: 10.7779 - val_loss: 271.6557 - val_mae: 9.7425\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 254.2208 - mae: 12.4718 - val_loss: 269.6633 - val_mae: 9.7360\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 180.1717 - mae: 11.4005 - val_loss: 268.2728 - val_mae: 9.7323\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 191.8959 - mae: 10.8366 - val_loss: 266.6119 - val_mae: 9.7001\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 216.3020 - mae: 12.8315 - val_loss: 265.0654 - val_mae: 9.6501\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 412.8841 - mae: 17.9780 - val_loss: 263.4713 - val_mae: 9.5733\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 359.7217 - mae: 15.6309 - val_loss: 262.1442 - val_mae: 9.5013\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 170.6937 - mae: 11.7063 - val_loss: 261.1202 - val_mae: 9.4267\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 199.8983 - mae: 10.3914 - val_loss: 260.0457 - val_mae: 9.3591\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 196.5595 - mae: 11.9317 - val_loss: 259.1965 - val_mae: 9.2909\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 136.0583 - mae: 9.7153 - val_loss: 258.2101 - val_mae: 9.2699\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 151.9690 - mae: 9.9788 - val_loss: 256.9559 - val_mae: 9.2456\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 193.3286 - mae: 10.6521 - val_loss: 255.8077 - val_mae: 9.2225\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 282.6185 - mae: 13.6679 - val_loss: 254.5145 - val_mae: 9.1951\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 199.6436 - mae: 12.0314 - val_loss: 253.4992 - val_mae: 9.1741\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 288.7333 - mae: 13.3036 - val_loss: 252.2976 - val_mae: 9.1574\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 254.5895 - mae: 10.3719 - val_loss: 250.9242 - val_mae: 9.1460\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 217.8960 - mae: 13.5823 - val_loss: 249.0444 - val_mae: 9.0984\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 243.0440 - mae: 12.1470 - val_loss: 247.3474 - val_mae: 9.0726\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 345.7326 - mae: 14.4231 - val_loss: 245.6619 - val_mae: 9.0634\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 110.1995 - mae: 9.5265 - val_loss: 243.4981 - val_mae: 9.0073\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 228.3015 - mae: 12.3217 - val_loss: 241.3133 - val_mae: 8.9533\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 156.4505 - mae: 10.5117 - val_loss: 238.8582 - val_mae: 8.8782\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 175.0520 - mae: 10.6473 - val_loss: 236.3520 - val_mae: 8.7965\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 201.1570 - mae: 11.7062 - val_loss: 233.7795 - val_mae: 8.7464\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 130.0588 - mae: 9.0781 - val_loss: 231.3759 - val_mae: 8.6814\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 150.3831 - mae: 8.6672 - val_loss: 229.0122 - val_mae: 8.6089\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 362.5191 - mae: 16.9738 - val_loss: 226.4364 - val_mae: 8.5413\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 262.4956 - mae: 12.1969 - val_loss: 223.7920 - val_mae: 8.4481\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 270.8510 - mae: 14.1255 - val_loss: 221.0483 - val_mae: 8.3575\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 262.2199 - mae: 14.6018 - val_loss: 218.2340 - val_mae: 8.2669\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 167.9049 - mae: 10.3507 - val_loss: 215.6959 - val_mae: 8.1931\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 103.3245 - mae: 7.5826 - val_loss: 213.7018 - val_mae: 8.1539\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 242.0399 - mae: 11.3722 - val_loss: 211.4826 - val_mae: 8.1166\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 138.0697 - mae: 10.2452 - val_loss: 209.5088 - val_mae: 8.0864\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 224.5555 - mae: 12.0201 - val_loss: 207.5041 - val_mae: 8.0544\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 54.1730 - mae: 6.2812 - val_loss: 205.3233 - val_mae: 8.0289\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 69.3766 - mae: 6.7560 - val_loss: 203.2294 - val_mae: 8.0054\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 232.5958 - mae: 10.4784 - val_loss: 201.4282 - val_mae: 7.9775\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 203.3378 - mae: 9.4234 - val_loss: 199.7977 - val_mae: 7.9426\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 185.8647 - mae: 9.9655 - val_loss: 198.5608 - val_mae: 7.9098\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 182.0234 - mae: 11.1848 - val_loss: 197.1877 - val_mae: 7.8855\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 211.5866 - mae: 11.1769 - val_loss: 196.3567 - val_mae: 7.8531\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 75.3587 - mae: 7.3531 - val_loss: 196.0369 - val_mae: 7.8134\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 199.3459 - mae: 12.4280 - val_loss: 195.7693 - val_mae: 7.7725\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 65.6143 - mae: 7.0285 - val_loss: 195.2744 - val_mae: 7.7371\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 98.2707 - mae: 6.8942 - val_loss: 194.9703 - val_mae: 7.7053\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 208.0554 - mae: 11.4712 - val_loss: 194.4882 - val_mae: 7.6732\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 108.5499 - mae: 7.5111 - val_loss: 194.1681 - val_mae: 7.6358\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 109.4687 - mae: 8.7291 - val_loss: 193.3720 - val_mae: 7.6056\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 278.5733 - mae: 13.8564 - val_loss: 193.0111 - val_mae: 7.5983\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 141.2311 - mae: 9.9978 - val_loss: 192.5285 - val_mae: 7.6210\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 164.9674 - mae: 10.9755 - val_loss: 192.2107 - val_mae: 7.6556\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 158.5523 - mae: 10.1656 - val_loss: 192.0780 - val_mae: 7.6966\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 251.1844 - mae: 12.4939 - val_loss: 191.5988 - val_mae: 7.7191\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 202.1776 - mae: 11.5367 - val_loss: 190.7385 - val_mae: 7.6937\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 152.5155 - mae: 10.5800 - val_loss: 190.6521 - val_mae: 7.7083\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 147.4243 - mae: 10.1584 - val_loss: 190.2364 - val_mae: 7.7390\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 189.5599 - mae: 12.5712 - val_loss: 189.8255 - val_mae: 7.7729\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 224.3621 - mae: 10.8460 - val_loss: 189.1203 - val_mae: 7.7582\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 142.9635 - mae: 9.8566 - val_loss: 188.1310 - val_mae: 7.7366\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 218.5126 - mae: 11.6362 - val_loss: 186.7505 - val_mae: 7.7488\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 158.4025 - mae: 10.2290 - val_loss: 185.7933 - val_mae: 7.7526\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 226.7566 - mae: 13.5143 - val_loss: 185.2359 - val_mae: 7.7369\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 236.9984 - mae: 12.2213 - val_loss: 185.0547 - val_mae: 7.7562\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 179.8759 - mae: 11.7303 - val_loss: 184.6005 - val_mae: 7.7474\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 159.5736 - mae: 11.1754 - val_loss: 183.9202 - val_mae: 7.7312\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 149.6768 - mae: 10.1332 - val_loss: 183.1487 - val_mae: 7.7021\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 98.1127 - mae: 8.8954 - val_loss: 182.2072 - val_mae: 7.6628\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 122.5944 - mae: 9.8740 - val_loss: 181.3226 - val_mae: 7.6196\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.2823 - mae: 7.1921 - val_loss: 180.1754 - val_mae: 7.5496\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 279.7317 - mae: 12.2040 - val_loss: 178.7199 - val_mae: 7.4592\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 105.5441 - mae: 8.9379 - val_loss: 177.3353 - val_mae: 7.3772\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 118.4023 - mae: 9.3518 - val_loss: 176.1520 - val_mae: 7.3078\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 80.5794 - mae: 7.0273 - val_loss: 174.9257 - val_mae: 7.2321\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 141.2531 - mae: 8.7790 - val_loss: 173.7946 - val_mae: 7.1784\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 231.0143 - mae: 12.3123 - val_loss: 172.4615 - val_mae: 7.1015\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 214.8766 - mae: 12.3387 - val_loss: 171.6543 - val_mae: 7.0492\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 121.6102 - mae: 8.6269 - val_loss: 170.8369 - val_mae: 7.0131\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 205.6062 - mae: 11.0159 - val_loss: 170.1345 - val_mae: 6.9343\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 76.5332 - mae: 6.4554 - val_loss: 169.5594 - val_mae: 6.8398\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 160.7464 - mae: 10.6849 - val_loss: 169.3369 - val_mae: 6.7446\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 225.3588 - mae: 12.1844 - val_loss: 168.7301 - val_mae: 6.7100\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 136.7269 - mae: 9.3831 - val_loss: 168.1159 - val_mae: 6.6856\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 263.7016 - mae: 14.2107 - val_loss: 167.9168 - val_mae: 6.6690\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 122.4213 - mae: 8.7467 - val_loss: 167.3592 - val_mae: 6.6714\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 73.8044 - mae: 6.5426 - val_loss: 166.6440 - val_mae: 6.6705\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 216.0816 - mae: 12.0507 - val_loss: 165.8492 - val_mae: 6.6784\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 220.9639 - mae: 12.1482 - val_loss: 164.9239 - val_mae: 6.6465\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 178.9044 - mae: 12.9822 - val_loss: 163.7370 - val_mae: 6.6323\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 231.7441 - mae: 12.4883 - val_loss: 162.2466 - val_mae: 6.6380\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 174.4836 - mae: 10.9451 - val_loss: 161.2204 - val_mae: 6.6412\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 115.5862 - mae: 9.5518 - val_loss: 160.6852 - val_mae: 6.6353\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 160.0368 - mae: 10.4560 - val_loss: 159.9045 - val_mae: 6.6343\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 330.5358 - mae: 14.2212 - val_loss: 158.9592 - val_mae: 6.5913\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 199.8544 - mae: 11.9025 - val_loss: 158.2927 - val_mae: 6.5835\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 254.1771 - mae: 11.8961 - val_loss: 157.8105 - val_mae: 6.5697\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 36.1303 - mae: 4.7757 - val_loss: 157.1083 - val_mae: 6.5476\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 91.8951 - mae: 8.0917 - val_loss: 156.1326 - val_mae: 6.5196\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 156.1637 - mae: 9.8898 - val_loss: 155.1960 - val_mae: 6.4901\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 126.4879 - mae: 9.4628 - val_loss: 154.2501 - val_mae: 6.4486\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 161.8076 - mae: 10.1094 - val_loss: 153.4736 - val_mae: 6.4180\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 91.5302 - mae: 8.3879 - val_loss: 152.8510 - val_mae: 6.4298\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 191.3429 - mae: 10.0969 - val_loss: 152.4512 - val_mae: 6.4333\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 139.6606 - mae: 10.2999 - val_loss: 152.3903 - val_mae: 6.4859\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 249.5843 - mae: 13.0001 - val_loss: 152.4857 - val_mae: 6.5247\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 133.2919 - mae: 8.6274 - val_loss: 152.4541 - val_mae: 6.5478\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 226.3660 - mae: 12.1456 - val_loss: 152.7099 - val_mae: 6.5775\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 136.8728 - mae: 9.8914 - val_loss: 152.5349 - val_mae: 6.6024\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 67.4958 - mae: 6.8473 - val_loss: 151.9249 - val_mae: 6.6180\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 202.4707 - mae: 11.5533 - val_loss: 151.4322 - val_mae: 6.6580\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 159.6029 - mae: 10.3151 - val_loss: 151.0775 - val_mae: 6.6936\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 213.1017 - mae: 11.6961 - val_loss: 150.9681 - val_mae: 6.6902\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 130.9703 - mae: 10.0488 - val_loss: 150.8526 - val_mae: 6.7023\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 206.7817 - mae: 12.0665 - val_loss: 150.5325 - val_mae: 6.7025\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 183.3062 - mae: 10.9946 - val_loss: 150.3207 - val_mae: 6.7232\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 227.1752 - mae: 12.3787 - val_loss: 149.9079 - val_mae: 6.7237\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 71.7836 - mae: 7.7399 - val_loss: 149.6534 - val_mae: 6.7357\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 108.6911 - mae: 8.9473 - val_loss: 149.2229 - val_mae: 6.7362\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 151.2694 - mae: 10.1182 - val_loss: 148.5759 - val_mae: 6.7216\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 66.6101 - mae: 6.5733 - val_loss: 148.0334 - val_mae: 6.7080\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 222.5713 - mae: 12.7414 - val_loss: 146.7827 - val_mae: 6.6607\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 221.6256 - mae: 11.6383 - val_loss: 145.2112 - val_mae: 6.6030\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 88.9231 - mae: 7.2257 - val_loss: 143.6404 - val_mae: 6.5450\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 67.8152 - mae: 6.5293 - val_loss: 141.9716 - val_mae: 6.4777\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 152.9571 - mae: 11.1478 - val_loss: 140.0185 - val_mae: 6.3915\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 279.1819 - mae: 13.4567 - val_loss: 138.4872 - val_mae: 6.3299\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 132.6963 - mae: 9.7272 - val_loss: 137.0379 - val_mae: 6.2702\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 121.6809 - mae: 9.0655 - val_loss: 135.7881 - val_mae: 6.2244\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 189.8664 - mae: 10.3901 - val_loss: 134.4592 - val_mae: 6.1792\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 190.0189 - mae: 12.0409 - val_loss: 133.0085 - val_mae: 6.1317\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 299.4340 - mae: 14.1795 - val_loss: 131.6049 - val_mae: 6.0769\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 67.6604 - mae: 7.0653 - val_loss: 130.5429 - val_mae: 6.0439\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 173.3561 - mae: 11.8115 - val_loss: 129.2595 - val_mae: 6.0341\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 163.5652 - mae: 10.8228 - val_loss: 127.8022 - val_mae: 6.0275\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 155.5720 - mae: 9.9888 - val_loss: 126.6157 - val_mae: 6.0368\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 56.7188 - mae: 5.8087 - val_loss: 125.4400 - val_mae: 6.0436\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 113.9505 - mae: 8.6685 - val_loss: 124.5449 - val_mae: 6.0447\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 166.6815 - mae: 10.8771 - val_loss: 123.1681 - val_mae: 5.9999\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 121.1936 - mae: 8.0660 - val_loss: 121.9695 - val_mae: 5.9425\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 96.3376 - mae: 7.8782 - val_loss: 120.8588 - val_mae: 5.8880\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 214.0415 - mae: 12.8600 - val_loss: 120.8022 - val_mae: 5.8540\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 164.9196 - mae: 10.2662 - val_loss: 121.2130 - val_mae: 5.8516\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 138.2424 - mae: 9.9064 - val_loss: 121.0827 - val_mae: 5.8485\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.5933 - mae: 6.9483 - val_loss: 120.9318 - val_mae: 5.8906\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 249.0193 - mae: 12.6784 - val_loss: 121.1130 - val_mae: 5.8827\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 232.8621 - mae: 10.6133 - val_loss: 121.0697 - val_mae: 5.8726\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 107.3851 - mae: 8.4392 - val_loss: 121.3222 - val_mae: 5.8722\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 258.5442 - mae: 12.2717 - val_loss: 121.4127 - val_mae: 5.8862\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 160.2475 - mae: 10.2564 - val_loss: 121.4796 - val_mae: 5.9011\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 101.2511 - mae: 8.1763 - val_loss: 121.4604 - val_mae: 5.9176\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 144.9528 - mae: 10.6326 - val_loss: 121.2472 - val_mae: 5.9214\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 54.6966 - mae: 5.9840 - val_loss: 121.0942 - val_mae: 5.9339\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 94.9115 - mae: 6.8905 - val_loss: 120.9991 - val_mae: 5.9520\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 65.6532 - mae: 7.2722 - val_loss: 121.1945 - val_mae: 5.9879\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 108.2551 - mae: 8.6013 - val_loss: 121.4779 - val_mae: 6.0303\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 107.9303 - mae: 7.6936 - val_loss: 121.4208 - val_mae: 6.0563\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 212.3935 - mae: 11.2163 - val_loss: 121.4336 - val_mae: 6.0796\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 101.1794 - mae: 7.7855 - val_loss: 121.4048 - val_mae: 6.0957\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 105.9694 - mae: 8.1376 - val_loss: 121.4319 - val_mae: 6.1211\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 189.3133 - mae: 11.3768 - val_loss: 121.0258 - val_mae: 6.1152\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 150.3675 - mae: 9.6742 - val_loss: 120.6437 - val_mae: 6.1083\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 147.9728 - mae: 9.1440 - val_loss: 120.1635 - val_mae: 6.1042\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 147.0793 - mae: 10.5607 - val_loss: 119.7887 - val_mae: 6.1070\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 113.8302 - mae: 9.0240 - val_loss: 119.6456 - val_mae: 6.1291\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 174.0438 - mae: 9.6293 - val_loss: 119.8356 - val_mae: 6.1598\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 89.3268 - mae: 6.7116 - val_loss: 120.2568 - val_mae: 6.2089\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 156.8942 - mae: 11.0260 - val_loss: 120.1949 - val_mae: 6.2283\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 251.3127 - mae: 12.7403 - val_loss: 119.8706 - val_mae: 6.2368\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 135.4786 - mae: 8.6639 - val_loss: 119.5718 - val_mae: 6.2398\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 165.8015 - mae: 9.8443 - val_loss: 119.2756 - val_mae: 6.2479\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 191.3444 - mae: 10.8078 - val_loss: 118.6859 - val_mae: 6.2281\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 164.5667 - mae: 10.4844 - val_loss: 117.9125 - val_mae: 6.2089\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 192.4959 - mae: 11.5352 - val_loss: 117.0957 - val_mae: 6.1849\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 176.9711 - mae: 11.8093 - val_loss: 116.5046 - val_mae: 6.1653\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 100.1918 - mae: 9.1914 - val_loss: 115.6986 - val_mae: 6.1284\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 63.9147 - mae: 6.6640 - val_loss: 114.8701 - val_mae: 6.0901\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 172.1311 - mae: 9.4056 - val_loss: 114.6629 - val_mae: 6.0840\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 118.1786 - mae: 8.6162 - val_loss: 114.5943 - val_mae: 6.1460\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 254.6472 - mae: 12.0468 - val_loss: 114.7383 - val_mae: 6.2118\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 196.8278 - mae: 12.5789 - val_loss: 114.5560 - val_mae: 6.2713\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 162.6182 - mae: 10.6967 - val_loss: 115.0782 - val_mae: 6.3682\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 107.7766 - mae: 7.8953 - val_loss: 115.3099 - val_mae: 6.4365\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 72.4641 - mae: 7.5069 - val_loss: 115.7805 - val_mae: 6.5266\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 123.3295 - mae: 9.3798 - val_loss: 115.7235 - val_mae: 6.5436\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 310.0078 - mae: 12.9959 - val_loss: 115.9727 - val_mae: 6.5965\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 43.8891 - mae: 4.9273 - val_loss: 116.3702 - val_mae: 6.6394\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 150.8218 - mae: 9.6519 - val_loss: 116.5243 - val_mae: 6.6637\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 265.2674 - mae: 12.8878 - val_loss: 116.4732 - val_mae: 6.6910\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 265.0609 - mae: 13.9071 - val_loss: 116.2807 - val_mae: 6.7172\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 297.2595 - mae: 13.9805 - val_loss: 116.1735 - val_mae: 6.7784\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 195.8870 - mae: 10.9184 - val_loss: 116.5107 - val_mae: 6.8654\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 92.4070 - mae: 7.8181 - val_loss: 117.2029 - val_mae: 6.9916\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 80.1286 - mae: 7.7003 - val_loss: 117.8424 - val_mae: 7.1119\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 114.2820 - mae: 8.1952 - val_loss: 118.3841 - val_mae: 7.1960\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 213.1980 - mae: 13.2424 - val_loss: 119.5064 - val_mae: 7.3124\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 114.4258 - mae: 8.9920 - val_loss: 120.1039 - val_mae: 7.3808\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 132.5992 - mae: 9.9104 - val_loss: 120.1592 - val_mae: 7.4044\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 113.1546 - mae: 9.0837 - val_loss: 119.5493 - val_mae: 7.3906\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 147.0667 - mae: 9.9080 - val_loss: 118.6083 - val_mae: 7.3418\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 148.2162 - mae: 8.8887 - val_loss: 117.7106 - val_mae: 7.2770\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 53.9848 - mae: 5.2991 - val_loss: 117.1430 - val_mae: 7.2472\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 112.8535 - mae: 9.2358 - val_loss: 116.4629 - val_mae: 7.2209\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 78.3954 - mae: 7.4235 - val_loss: 115.4412 - val_mae: 7.1715\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 87.1707 - mae: 7.7406 - val_loss: 113.9307 - val_mae: 7.0937\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 96.8860 - mae: 7.8578 - val_loss: 112.4850 - val_mae: 7.0173\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 111.3312 - mae: 8.3460 - val_loss: 111.1061 - val_mae: 6.9387\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 95.3417 - mae: 7.4938 - val_loss: 109.7181 - val_mae: 6.8600\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 109.4134 - mae: 7.6848 - val_loss: 108.5075 - val_mae: 6.7844\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 172.6942 - mae: 10.3546 - val_loss: 107.6967 - val_mae: 6.7652\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 79.4633 - mae: 7.4664 - val_loss: 107.2451 - val_mae: 6.7670\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 123.8429 - mae: 8.4391 - val_loss: 106.8783 - val_mae: 6.7682\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.9880 - mae: 7.9492 - val_loss: 106.4501 - val_mae: 6.7718\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 160.9322 - mae: 10.8561 - val_loss: 106.0400 - val_mae: 6.7854\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 183.9406 - mae: 10.9308 - val_loss: 105.3119 - val_mae: 6.7754\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 180.3772 - mae: 11.0527 - val_loss: 103.8798 - val_mae: 6.6992\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 143.2613 - mae: 10.3135 - val_loss: 102.3516 - val_mae: 6.6186\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 146.7977 - mae: 11.2662 - val_loss: 100.8762 - val_mae: 6.5452\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 182.1101 - mae: 10.6453 - val_loss: 99.8275 - val_mae: 6.4971\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 111.6304 - mae: 9.0121 - val_loss: 99.0823 - val_mae: 6.4544\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 166.9409 - mae: 9.5378 - val_loss: 98.0388 - val_mae: 6.3679\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 202.6378 - mae: 12.0508 - val_loss: 96.5363 - val_mae: 6.2393\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 149.2987 - mae: 9.5571 - val_loss: 95.0835 - val_mae: 6.0840\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 168.4273 - mae: 10.6623 - val_loss: 93.4221 - val_mae: 5.9236\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 196.8183 - mae: 11.5328 - val_loss: 92.3558 - val_mae: 5.8336\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 128.1087 - mae: 8.7175 - val_loss: 91.8686 - val_mae: 5.7830\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 59.9963 - mae: 6.2734 - val_loss: 91.0874 - val_mae: 5.7021\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 72.4614 - mae: 7.7132 - val_loss: 90.5675 - val_mae: 5.6470\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 97.2687 - mae: 8.4082 - val_loss: 90.2503 - val_mae: 5.6335\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 120.7863 - mae: 9.0938 - val_loss: 89.9819 - val_mae: 5.6234\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 132.3334 - mae: 8.5621 - val_loss: 89.3578 - val_mae: 5.5871\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 121.0380 - mae: 9.0180 - val_loss: 89.0069 - val_mae: 5.5774\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 137.5785 - mae: 9.8344 - val_loss: 88.6577 - val_mae: 5.5587\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 153.1761 - mae: 10.2608 - val_loss: 88.6935 - val_mae: 5.5848\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 103.0652 - mae: 7.1386 - val_loss: 88.7925 - val_mae: 5.6052\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 143.7811 - mae: 9.5006 - val_loss: 88.4346 - val_mae: 5.5609\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 114.5145 - mae: 8.2792 - val_loss: 88.3004 - val_mae: 5.5266\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 105.1784 - mae: 7.6284 - val_loss: 88.2696 - val_mae: 5.5079\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 99.2656 - mae: 7.2928 - val_loss: 87.8039 - val_mae: 5.4440\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 138.6254 - mae: 8.6047 - val_loss: 87.4872 - val_mae: 5.4182\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 145.9516 - mae: 9.6055 - val_loss: 87.1432 - val_mae: 5.4085\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 93.0294 - mae: 7.9769 - val_loss: 86.8188 - val_mae: 5.3742\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 64.4786 - mae: 6.5646 - val_loss: 86.5504 - val_mae: 5.3360\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 215.1343 - mae: 11.2041 - val_loss: 85.6046 - val_mae: 5.2073\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 151.3080 - mae: 9.3747 - val_loss: 84.9959 - val_mae: 5.1887\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 113.7525 - mae: 6.8393 - val_loss: 84.2903 - val_mae: 5.1645\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 121.5398 - mae: 9.3429 - val_loss: 83.2701 - val_mae: 5.1110\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 192.8689 - mae: 10.0033 - val_loss: 82.1001 - val_mae: 5.0425\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 64.8642 - mae: 6.4074 - val_loss: 80.9954 - val_mae: 5.0368\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 139.8583 - mae: 7.7842 - val_loss: 80.0132 - val_mae: 5.0486\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 136.6082 - mae: 10.7182 - val_loss: 78.8764 - val_mae: 5.0640\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 149.6819 - mae: 8.9451 - val_loss: 77.9528 - val_mae: 5.0959\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 173.2799 - mae: 10.8923 - val_loss: 77.1412 - val_mae: 5.1238\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 208.0484 - mae: 11.0596 - val_loss: 76.5955 - val_mae: 5.1321\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 70.2422 - mae: 6.6506 - val_loss: 76.0786 - val_mae: 5.1581\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 85.1091 - mae: 7.9274 - val_loss: 75.8553 - val_mae: 5.1813\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 93.2958 - mae: 8.0785 - val_loss: 75.6906 - val_mae: 5.1758\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 232.8483 - mae: 13.1813 - val_loss: 75.6253 - val_mae: 5.2015\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 80.8542 - mae: 8.0392 - val_loss: 75.4421 - val_mae: 5.2341\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 62.4269 - mae: 6.4513 - val_loss: 75.2465 - val_mae: 5.2798\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 132.3056 - mae: 10.4316 - val_loss: 74.9919 - val_mae: 5.2982\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 121.9039 - mae: 9.1742 - val_loss: 74.9139 - val_mae: 5.3222\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 100.6104 - mae: 8.3930 - val_loss: 75.1227 - val_mae: 5.3661\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 183.9816 - mae: 11.1685 - val_loss: 75.3894 - val_mae: 5.4418\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 115.6653 - mae: 7.9010 - val_loss: 75.6839 - val_mae: 5.5046\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 69.0356 - mae: 6.4901 - val_loss: 75.6382 - val_mae: 5.5228\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 177.0095 - mae: 9.2597 - val_loss: 75.8384 - val_mae: 5.5950\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 15.9757 - mae: 3.0140\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Mean Squared Error: 15.975674629211426\n",
            "Mean Absolute Error: 3.0140304565429688\n",
            "R-squared: 0.6309627058281735\n",
            "Adjusted R-squared: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d6ee8bdd2c46>:23: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NN Trial 3"
      ],
      "metadata": {
        "id": "nwizZGJt7hRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=4, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Use a different optimizer and compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# Use early stopping during training\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "\n",
        "# Train the model with more epochs and use early stopping\n",
        "history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=16, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse, mae = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "n = X_test_scaled.shape[0]  # Number of samples\n",
        "p = X_test_scaled.shape[1]  # Number of features\n",
        "adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print('R-squared:', r2)\n",
        "print('Adjusted R-squared:', adj_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk0Y5pife-p7",
        "outputId": "8c6ec3d9-6ff1-4e42-c6e3-b4a7f04177c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9533.6338 - mae: 97.2640 - val_loss: 9952.1328 - val_mae: 99.6662\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9487.5352 - mae: 97.0456 - val_loss: 9944.0186 - val_mae: 99.6256\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 9462.3291 - mae: 96.9048 - val_loss: 9935.6602 - val_mae: 99.5836\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9513.2988 - mae: 97.1740 - val_loss: 9927.9912 - val_mae: 99.5452\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 9499.8027 - mae: 97.0895 - val_loss: 9920.5273 - val_mae: 99.5078\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 9477.6182 - mae: 96.9772 - val_loss: 9913.2207 - val_mae: 99.4711\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9490.1006 - mae: 97.0309 - val_loss: 9905.8730 - val_mae: 99.4342\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9429.2480 - mae: 96.7412 - val_loss: 9898.0049 - val_mae: 99.3945\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9442.1582 - mae: 96.8023 - val_loss: 9890.0039 - val_mae: 99.3542\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9442.3555 - mae: 96.7977 - val_loss: 9881.7168 - val_mae: 99.3123\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9424.3057 - mae: 96.7256 - val_loss: 9873.1152 - val_mae: 99.2689\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9429.4395 - mae: 96.7220 - val_loss: 9864.3574 - val_mae: 99.2247\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9378.0332 - mae: 96.4544 - val_loss: 9855.2529 - val_mae: 99.1788\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9393.5469 - mae: 96.5491 - val_loss: 9845.7910 - val_mae: 99.1309\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9429.4385 - mae: 96.7203 - val_loss: 9835.9746 - val_mae: 99.0813\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 9318.4111 - mae: 96.1859 - val_loss: 9825.4434 - val_mae: 99.0281\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9365.8213 - mae: 96.3962 - val_loss: 9813.9492 - val_mae: 98.9700\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9347.2646 - mae: 96.3047 - val_loss: 9801.5273 - val_mae: 98.9073\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9280.1133 - mae: 95.9316 - val_loss: 9788.3643 - val_mae: 98.8408\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9350.2744 - mae: 96.3075 - val_loss: 9774.8379 - val_mae: 98.7723\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9313.4238 - mae: 96.1394 - val_loss: 9760.7480 - val_mae: 98.7009\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9255.7832 - mae: 95.8403 - val_loss: 9745.6230 - val_mae: 98.6243\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9228.5762 - mae: 95.6694 - val_loss: 9729.2568 - val_mae: 98.5414\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9136.1865 - mae: 95.1580 - val_loss: 9711.5127 - val_mae: 98.4513\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9213.1045 - mae: 95.5725 - val_loss: 9692.7930 - val_mae: 98.3561\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9198.0459 - mae: 95.5296 - val_loss: 9673.1182 - val_mae: 98.2560\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9128.1367 - mae: 95.1726 - val_loss: 9652.2148 - val_mae: 98.1496\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9154.1660 - mae: 95.2911 - val_loss: 9630.3271 - val_mae: 98.0379\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9096.8887 - mae: 95.0024 - val_loss: 9606.3975 - val_mae: 97.9157\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9183.8828 - mae: 95.3987 - val_loss: 9581.0625 - val_mae: 97.7862\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9061.7451 - mae: 94.7979 - val_loss: 9554.2363 - val_mae: 97.6488\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9091.7871 - mae: 94.9452 - val_loss: 9525.9277 - val_mae: 97.5036\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9091.3916 - mae: 94.9938 - val_loss: 9496.2158 - val_mae: 97.3508\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9057.1133 - mae: 94.7619 - val_loss: 9465.3086 - val_mae: 97.1915\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8953.5371 - mae: 94.2262 - val_loss: 9432.6309 - val_mae: 97.0229\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8911.8838 - mae: 94.0438 - val_loss: 9397.8633 - val_mae: 96.8431\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 8932.6465 - mae: 94.1014 - val_loss: 9360.9004 - val_mae: 96.6517\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8858.9570 - mae: 93.7081 - val_loss: 9321.9619 - val_mae: 96.4496\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8842.2188 - mae: 93.5899 - val_loss: 9280.6738 - val_mae: 96.2347\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 8919.5371 - mae: 94.0016 - val_loss: 9238.0879 - val_mae: 96.0123\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8784.0645 - mae: 93.2972 - val_loss: 9193.3906 - val_mae: 95.7783\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8719.4834 - mae: 92.8959 - val_loss: 9145.8848 - val_mae: 95.5289\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8669.5400 - mae: 92.6085 - val_loss: 9095.4395 - val_mae: 95.2634\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8511.9199 - mae: 91.7935 - val_loss: 9041.3418 - val_mae: 94.9779\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8352.7871 - mae: 90.8461 - val_loss: 8983.5527 - val_mae: 94.6720\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8325.7900 - mae: 90.6400 - val_loss: 8922.2803 - val_mae: 94.3463\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8179.8467 - mae: 90.0506 - val_loss: 8856.8184 - val_mae: 93.9973\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8334.9395 - mae: 90.8230 - val_loss: 8788.1279 - val_mae: 93.6297\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8277.5830 - mae: 90.5276 - val_loss: 8715.8867 - val_mae: 93.2415\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8283.4648 - mae: 90.6099 - val_loss: 8640.3359 - val_mae: 92.8337\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 8048.3301 - mae: 89.2721 - val_loss: 8560.7080 - val_mae: 92.4020\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7727.9844 - mae: 87.1870 - val_loss: 8476.0869 - val_mae: 91.9406\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7987.8931 - mae: 88.8229 - val_loss: 8387.3252 - val_mae: 91.4540\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7630.7441 - mae: 86.3889 - val_loss: 8294.0684 - val_mae: 90.9395\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7759.0156 - mae: 87.5016 - val_loss: 8196.3975 - val_mae: 90.3974\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7154.7139 - mae: 84.1070 - val_loss: 8092.5283 - val_mae: 89.8173\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7196.6348 - mae: 84.1166 - val_loss: 7983.2808 - val_mae: 89.2028\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7709.2065 - mae: 87.2029 - val_loss: 7870.8008 - val_mae: 88.5654\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7362.3213 - mae: 85.1560 - val_loss: 7753.2510 - val_mae: 87.8942\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7451.6807 - mae: 85.8090 - val_loss: 7631.4473 - val_mae: 87.1927\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7091.3091 - mae: 83.7563 - val_loss: 7504.4702 - val_mae: 86.4554\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6914.6855 - mae: 82.4622 - val_loss: 7371.9453 - val_mae: 85.6787\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6757.3364 - mae: 81.4846 - val_loss: 7233.6133 - val_mae: 84.8598\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6500.6094 - mae: 79.8044 - val_loss: 7089.0137 - val_mae: 83.9945\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6163.1152 - mae: 76.8733 - val_loss: 6938.4531 - val_mae: 83.0827\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6578.9741 - mae: 79.6761 - val_loss: 6783.5244 - val_mae: 82.1326\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5496.6826 - mae: 72.7916 - val_loss: 6621.5312 - val_mae: 81.1265\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6451.7451 - mae: 78.9353 - val_loss: 6455.6851 - val_mae: 80.0819\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5892.6997 - mae: 75.8704 - val_loss: 6283.2051 - val_mae: 78.9794\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 5411.4268 - mae: 71.7312 - val_loss: 6104.1787 - val_mae: 77.8169\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5700.9712 - mae: 74.7451 - val_loss: 5919.8398 - val_mae: 76.6007\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5883.2939 - mae: 76.0219 - val_loss: 5732.0649 - val_mae: 75.3396\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4659.0894 - mae: 66.4645 - val_loss: 5538.3774 - val_mae: 74.0148\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5566.7510 - mae: 72.9559 - val_loss: 5342.2627 - val_mae: 72.6455\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 4686.1973 - mae: 65.8705 - val_loss: 5142.3481 - val_mae: 71.2205\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4975.9907 - mae: 69.6891 - val_loss: 4938.9219 - val_mae: 69.7383\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 5092.1465 - mae: 70.3348 - val_loss: 4733.2393 - val_mae: 68.2030\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3899.5901 - mae: 58.1056 - val_loss: 4524.3853 - val_mae: 66.6042\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 4458.6831 - mae: 62.1738 - val_loss: 4314.9312 - val_mae: 64.9583\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3530.6094 - mae: 53.4050 - val_loss: 4104.6294 - val_mae: 63.2594\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 3920.4006 - mae: 59.3733 - val_loss: 3893.0813 - val_mae: 61.4987\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2309.2620 - mae: 43.0394 - val_loss: 3681.0415 - val_mae: 59.6789\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3218.3545 - mae: 54.0020 - val_loss: 3468.5859 - val_mae: 57.7917\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2492.1206 - mae: 46.0775 - val_loss: 3256.6499 - val_mae: 55.8404\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2699.8904 - mae: 49.0835 - val_loss: 3046.3503 - val_mae: 53.8251\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2921.3188 - mae: 50.7304 - val_loss: 2841.1086 - val_mae: 51.7793\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2355.2952 - mae: 45.5180 - val_loss: 2643.3394 - val_mae: 49.7281\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1847.5562 - mae: 34.5426 - val_loss: 2453.9519 - val_mae: 47.6855\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2310.2217 - mae: 42.6351 - val_loss: 2269.5652 - val_mae: 45.6016\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1779.2449 - mae: 33.4243 - val_loss: 2093.0010 - val_mae: 43.5107\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1699.5532 - mae: 35.7130 - val_loss: 1923.0574 - val_mae: 41.3884\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1496.9792 - mae: 33.0309 - val_loss: 1764.3273 - val_mae: 39.2982\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2043.3599 - mae: 40.9940 - val_loss: 1613.9912 - val_mae: 37.2149\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1836.1385 - mae: 37.2447 - val_loss: 1476.8234 - val_mae: 35.2061\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1249.3108 - mae: 29.7347 - val_loss: 1348.8099 - val_mae: 33.2203\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1594.7076 - mae: 33.7845 - val_loss: 1229.3550 - val_mae: 31.2433\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2404.8044 - mae: 43.6505 - val_loss: 1123.2323 - val_mae: 29.4103\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1594.8269 - mae: 35.4643 - val_loss: 1029.5874 - val_mae: 27.6690\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1076.1544 - mae: 28.6405 - val_loss: 948.8224 - val_mae: 26.0837\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1051.9442 - mae: 27.1336 - val_loss: 873.2125 - val_mae: 24.4784\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1850.8015 - mae: 34.8567 - val_loss: 806.9532 - val_mae: 23.0416\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 864.7341 - mae: 21.1527 - val_loss: 746.5928 - val_mae: 21.6393\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1029.6272 - mae: 27.9627 - val_loss: 692.8315 - val_mae: 20.3062\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1170.5753 - mae: 29.1470 - val_loss: 643.0445 - val_mae: 18.9776\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 772.1552 - mae: 23.2523 - val_loss: 599.9680 - val_mae: 17.8907\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1296.3646 - mae: 29.8547 - val_loss: 563.2333 - val_mae: 16.8774\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 887.3668 - mae: 26.3516 - val_loss: 529.1736 - val_mae: 15.8794\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1452.6610 - mae: 28.8588 - val_loss: 504.6428 - val_mae: 14.9493\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1248.8248 - mae: 30.0238 - val_loss: 484.9780 - val_mae: 14.1130\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 557.7653 - mae: 20.1343 - val_loss: 466.5165 - val_mae: 13.2976\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1280.1556 - mae: 32.3590 - val_loss: 456.1734 - val_mae: 12.7309\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1363.1029 - mae: 31.1641 - val_loss: 445.0724 - val_mae: 12.1455\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1744.6157 - mae: 38.3853 - val_loss: 437.7697 - val_mae: 11.8683\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1601.2208 - mae: 31.7002 - val_loss: 433.2144 - val_mae: 11.7684\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1696.0037 - mae: 38.6285 - val_loss: 432.1339 - val_mae: 11.9180\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 917.1744 - mae: 23.9482 - val_loss: 433.1351 - val_mae: 12.1172\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1315.0276 - mae: 28.9014 - val_loss: 438.9397 - val_mae: 12.5731\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1323.0288 - mae: 31.0918 - val_loss: 447.9092 - val_mae: 13.1491\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1198.1954 - mae: 31.9747 - val_loss: 457.6389 - val_mae: 13.7183\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 614.9185 - mae: 20.0309 - val_loss: 467.2541 - val_mae: 14.2250\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1126.6086 - mae: 28.0172 - val_loss: 473.5205 - val_mae: 14.5618\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1750.5173 - mae: 30.3702 - val_loss: 484.8254 - val_mae: 15.0388\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1702.1479 - mae: 36.9958 - val_loss: 498.5068 - val_mae: 15.6310\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1265.3170 - mae: 31.5304 - val_loss: 513.0021 - val_mae: 16.1626\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1109.8407 - mae: 29.6398 - val_loss: 528.0009 - val_mae: 16.7130\n",
            "Epoch 125: early stopping\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 135.1569 - mae: 8.4678\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Mean Squared Error: 135.15692138671875\n",
            "Mean Absolute Error: 8.467778205871582\n",
            "R-squared: -2.169721702785538\n",
            "Adjusted R-squared: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-2bd1b7181ec7>:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NN Trial 4"
      ],
      "metadata": {
        "id": "mxjJoAoQ7jbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, input_dim=4, activation='relu'),\n",
        "    Dropout(0.2),  # Add dropout layer\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),  # Add dropout layer\n",
        "    Dense(16, activation='relu'),  # Add additional hidden layer\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Define the optimizer and compile the model\n",
        "optimizer = Adam(lr=0.0001)  # Adjust learning rate\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse, mae = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "n = X_test_scaled.shape[0]  # Number of samples\n",
        "p = X_test_scaled.shape[1]  # Number of features\n",
        "adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print('R-squared:', r2)\n",
        "print('Adjusted R-squared:', adj_r2)"
      ],
      "metadata": {
        "id": "9nYhmTcafUKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64999ef4-e2e2-4a66-9db5-1c577eceea8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 807ms/step - loss: 9522.0146 - mae: 97.1980 - val_loss: 9959.1182 - val_mae: 99.6956\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9502.8428 - mae: 97.1027 - val_loss: 9953.3408 - val_mae: 99.6665\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9491.3916 - mae: 97.0368 - val_loss: 9947.4121 - val_mae: 99.6366\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9502.7568 - mae: 97.1073 - val_loss: 9941.1523 - val_mae: 99.6051\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9482.1494 - mae: 96.9926 - val_loss: 9934.7734 - val_mae: 99.5731\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9469.5273 - mae: 96.9299 - val_loss: 9928.2988 - val_mae: 99.5405\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9469.8994 - mae: 96.9325 - val_loss: 9921.8418 - val_mae: 99.5079\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9461.8887 - mae: 96.8846 - val_loss: 9915.1592 - val_mae: 99.4742\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9452.3975 - mae: 96.8490 - val_loss: 9908.1348 - val_mae: 99.4387\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9440.3379 - mae: 96.7775 - val_loss: 9900.7402 - val_mae: 99.4013\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9440.7666 - mae: 96.7788 - val_loss: 9893.4688 - val_mae: 99.3645\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9434.0303 - mae: 96.7474 - val_loss: 9885.0215 - val_mae: 99.3220\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9413.2520 - mae: 96.6378 - val_loss: 9875.8418 - val_mae: 99.2759\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9417.9150 - mae: 96.6577 - val_loss: 9865.7764 - val_mae: 99.2254\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9371.2920 - mae: 96.4212 - val_loss: 9855.5029 - val_mae: 99.1739\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9423.1465 - mae: 96.6878 - val_loss: 9844.6436 - val_mae: 99.1193\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9364.8613 - mae: 96.3883 - val_loss: 9833.3213 - val_mae: 99.0624\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9373.4619 - mae: 96.4169 - val_loss: 9821.0596 - val_mae: 99.0009\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9348.5107 - mae: 96.3004 - val_loss: 9807.8965 - val_mae: 98.9348\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9335.8857 - mae: 96.2379 - val_loss: 9793.7012 - val_mae: 98.8633\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9325.1963 - mae: 96.1779 - val_loss: 9779.1455 - val_mae: 98.7900\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9312.8525 - mae: 96.1074 - val_loss: 9764.1387 - val_mae: 98.7142\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9281.7803 - mae: 95.9516 - val_loss: 9748.2881 - val_mae: 98.6342\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9279.5449 - mae: 95.9363 - val_loss: 9731.7432 - val_mae: 98.5506\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9281.8193 - mae: 95.9569 - val_loss: 9714.4785 - val_mae: 98.4632\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9243.0303 - mae: 95.7469 - val_loss: 9696.4824 - val_mae: 98.3721\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9204.8496 - mae: 95.5253 - val_loss: 9677.5928 - val_mae: 98.2764\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9193.7373 - mae: 95.4787 - val_loss: 9657.4854 - val_mae: 98.1745\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9166.2803 - mae: 95.3266 - val_loss: 9636.7061 - val_mae: 98.0691\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9167.3691 - mae: 95.3628 - val_loss: 9615.0732 - val_mae: 97.9592\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9135.8467 - mae: 95.2008 - val_loss: 9592.5605 - val_mae: 97.8447\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9108.5312 - mae: 95.0498 - val_loss: 9569.1572 - val_mae: 97.7256\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9080.0059 - mae: 94.8865 - val_loss: 9544.8545 - val_mae: 97.6017\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9064.4600 - mae: 94.7959 - val_loss: 9519.3867 - val_mae: 97.4716\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9000.3486 - mae: 94.4650 - val_loss: 9492.3115 - val_mae: 97.3333\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9017.0352 - mae: 94.5889 - val_loss: 9463.3652 - val_mae: 97.1854\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8923.3047 - mae: 94.0633 - val_loss: 9433.2129 - val_mae: 97.0311\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8903.4678 - mae: 93.9234 - val_loss: 9401.9707 - val_mae: 96.8709\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 8966.6553 - mae: 94.2839 - val_loss: 9369.4170 - val_mae: 96.7035\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8821.4062 - mae: 93.5190 - val_loss: 9335.0859 - val_mae: 96.5266\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8808.6826 - mae: 93.4707 - val_loss: 9299.1895 - val_mae: 96.3413\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8849.0176 - mae: 93.6262 - val_loss: 9261.8887 - val_mae: 96.1484\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8750.8076 - mae: 93.0948 - val_loss: 9223.1250 - val_mae: 95.9474\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8696.9727 - mae: 92.8397 - val_loss: 9182.6152 - val_mae: 95.7370\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8576.5264 - mae: 92.2186 - val_loss: 9140.1230 - val_mae: 95.5158\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8582.0264 - mae: 92.2718 - val_loss: 9095.9805 - val_mae: 95.2855\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8515.0293 - mae: 91.8707 - val_loss: 9049.9229 - val_mae: 95.0446\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8625.3447 - mae: 92.4246 - val_loss: 9002.5117 - val_mae: 94.7959\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8404.3584 - mae: 91.3165 - val_loss: 8952.7236 - val_mae: 94.5341\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8325.3232 - mae: 90.8372 - val_loss: 8900.6357 - val_mae: 94.2595\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8301.4248 - mae: 90.6972 - val_loss: 8846.2490 - val_mae: 93.9720\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8279.8633 - mae: 90.5825 - val_loss: 8789.7773 - val_mae: 93.6725\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8208.2812 - mae: 90.1241 - val_loss: 8730.4375 - val_mae: 93.3565\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 8099.7393 - mae: 89.5825 - val_loss: 8668.8262 - val_mae: 93.0272\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8092.6465 - mae: 89.5305 - val_loss: 8603.9248 - val_mae: 92.6786\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8117.1660 - mae: 89.6494 - val_loss: 8536.9463 - val_mae: 92.3174\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7987.0566 - mae: 88.8082 - val_loss: 8467.7676 - val_mae: 91.9426\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7892.8125 - mae: 88.3697 - val_loss: 8396.1143 - val_mae: 91.5526\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7822.1035 - mae: 88.0517 - val_loss: 8321.4648 - val_mae: 91.1443\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 7721.0830 - mae: 87.4260 - val_loss: 8244.3008 - val_mae: 90.7202\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7737.9160 - mae: 87.6262 - val_loss: 8164.6919 - val_mae: 90.2805\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 7524.0757 - mae: 86.3204 - val_loss: 8082.2451 - val_mae: 89.8227\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7457.5659 - mae: 85.9306 - val_loss: 7997.1904 - val_mae: 89.3478\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7438.5054 - mae: 85.8544 - val_loss: 7909.5308 - val_mae: 88.8554\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7256.5947 - mae: 84.7784 - val_loss: 7818.9150 - val_mae: 88.3434\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7186.5513 - mae: 84.1393 - val_loss: 7725.6729 - val_mae: 87.8130\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7104.9541 - mae: 83.7845 - val_loss: 7629.7632 - val_mae: 87.2638\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7071.5156 - mae: 83.6022 - val_loss: 7531.2847 - val_mae: 86.6959\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6620.7666 - mae: 80.8929 - val_loss: 7429.2954 - val_mae: 86.1034\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6635.6226 - mae: 81.0137 - val_loss: 7324.2344 - val_mae: 85.4885\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6594.0830 - mae: 80.7134 - val_loss: 7216.0918 - val_mae: 84.8505\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6523.2490 - mae: 80.3322 - val_loss: 7105.0117 - val_mae: 84.1896\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6674.5625 - mae: 81.0808 - val_loss: 6991.8042 - val_mae: 83.5100\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6339.6191 - mae: 79.1148 - val_loss: 6875.9326 - val_mae: 82.8082\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6080.4639 - mae: 77.4167 - val_loss: 6757.1758 - val_mae: 82.0821\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6000.0459 - mae: 76.8517 - val_loss: 6635.6685 - val_mae: 81.3318\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6018.1250 - mae: 76.8185 - val_loss: 6511.7080 - val_mae: 80.5583\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5560.2378 - mae: 73.9509 - val_loss: 6384.6968 - val_mae: 79.7574\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5857.7910 - mae: 76.1000 - val_loss: 6255.3105 - val_mae: 78.9324\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5931.0024 - mae: 76.2978 - val_loss: 6124.3428 - val_mae: 78.0874\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5719.6196 - mae: 74.9308 - val_loss: 5991.5869 - val_mae: 77.2204\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5205.3618 - mae: 71.5822 - val_loss: 5856.2510 - val_mae: 76.3253\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5027.2642 - mae: 70.2428 - val_loss: 5718.2090 - val_mae: 75.4003\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4990.5713 - mae: 70.1576 - val_loss: 5578.0176 - val_mae: 74.4480\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5072.4893 - mae: 70.1442 - val_loss: 5436.1963 - val_mae: 73.4706\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4541.1143 - mae: 66.8475 - val_loss: 5291.7505 - val_mae: 72.4602\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 4733.5859 - mae: 68.0986 - val_loss: 5145.7979 - val_mae: 71.4233\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4578.8135 - mae: 66.5929 - val_loss: 4998.7236 - val_mae: 70.3612\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 4036.1851 - mae: 62.3196 - val_loss: 4849.9736 - val_mae: 69.2688\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4261.9478 - mae: 64.2802 - val_loss: 4700.0166 - val_mae: 68.1477\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 4152.0239 - mae: 63.6950 - val_loss: 4548.5879 - val_mae: 66.9947\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 4001.7434 - mae: 62.0009 - val_loss: 4396.5347 - val_mae: 65.8141\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3656.8213 - mae: 59.1805 - val_loss: 4243.6899 - val_mae: 64.6031\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3656.5161 - mae: 59.3400 - val_loss: 4090.2131 - val_mae: 63.3606\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3194.4363 - mae: 55.2119 - val_loss: 3936.0972 - val_mae: 62.0856\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3276.9001 - mae: 55.3570 - val_loss: 3782.1028 - val_mae: 60.7820\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3587.2686 - mae: 58.6905 - val_loss: 3628.3491 - val_mae: 59.4484\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2921.3606 - mae: 52.9421 - val_loss: 3475.0984 - val_mae: 58.0847\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2673.2881 - mae: 49.6228 - val_loss: 3323.0894 - val_mae: 56.6953\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2852.2671 - mae: 51.0041 - val_loss: 3172.4563 - val_mae: 55.2795\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1938.0570 - mae: 41.9523\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Mean Squared Error: 1938.0570068359375\n",
            "Mean Absolute Error: 41.95233917236328\n",
            "R-squared: -44.45162271229136\n",
            "Adjusted R-squared: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-78ce236eb788>:24: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the LASSO model produces the best results, and the OLS model also performs quite well. In this case, OLS could be used over LASSO due to simplicity. \n",
        "\n",
        "The neural network model variations all do not perform well at all. This could possibly be due to the limited data size. "
      ],
      "metadata": {
        "id": "O3LldlXO68Rs"
      }
    }
  ]
}